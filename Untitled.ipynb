{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f95243d9080>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\" Base class \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self._parameters = OrderedDict()\n",
    "        self._children = OrderedDict()\n",
    "        self.training = True\n",
    "        \n",
    "    def __call__(self, *input, **kwargs):\n",
    "        return self.forward(*input, **kwargs)\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        \"\"\" backward receives as input a pointer to a tensor or a tuple of tensors containing\n",
    "        the gradient of the loss (or the function of interest) wrt the module's output, accumulates\n",
    "        the gradient wrt the parameters, and returns a tensor or a tuple of tensors containing the \n",
    "        gradient of the loss wrt the module's input (Application of the chain rule)\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_children(self, module):\n",
    "        print(\"adding child = \", module)\n",
    "        assert isinstance(module, Module) and module is not None, \"Not a Module.\"\n",
    "        assert module.name not in self._children, \"Module {} already exists\".format(module.name)\n",
    "        self._children[module.name] = module\n",
    "        \n",
    "    def add_parameter(self, name, param):\n",
    "        assert isinstance(param, Parameter), \"Not a Parameter.\"\n",
    "        assert name not in self._parameters, \"Parameter {} already exists\".format(name)\n",
    "        self._parameters[name] = param\n",
    "        \n",
    "    def param(self, recurse=True):\n",
    "        \"\"\" param returns a list of Parameters, each composed of a parameter tensor, \n",
    "        and a gradient tensor of same size. This list is empty for parameterless modules. \"\"\"\n",
    "        if recurse == False or self._children is not None:\n",
    "            print(\"Arrived in leaf module\")\n",
    "            return self.param_per_module()\n",
    "        else:\n",
    "            for key_mod, module in self._children.items():\n",
    "                print(\"Looping over children, module = \", module)\n",
    "                for key_param, parameter in module._parameters:\n",
    "                    return param_per_module()\n",
    "                    \n",
    "    \n",
    "    def param_per_module(self):\n",
    "        if self._parameters:\n",
    "            yield self._parameters\n",
    "        else:\n",
    "            yield None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__('NN')\n",
    "        for index, module in enumerate(args):\n",
    "            self.add_children(module)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        print(\"In Sequential.forward\")\n",
    "        for key, module in self._children.items():\n",
    "            input = module(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Module may have tensor parameters, for each of which it should also have a \n",
    "# similar sized tensor gradient to accumulate the gradient during the backward pass\n",
    "class Parameter(object):\n",
    "    def __init__(self, tensor=None, grad=None, requires_grad=True):\n",
    "        self.data = tensor\n",
    "        self.grad = torch.empty(tensor.size())\n",
    "        self.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\" Implements a R^C -> R^D fully-connected layer:\n",
    "        Input: (N x C) tensor\n",
    "        Ouput: (N x D) tensor \"\"\"\n",
    "    def __init__(self, name, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__(name)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        self.reset_parameters()\n",
    "        self.add_parameter('weight', self.weight)\n",
    "        self.add_parameter('bias', self.bias)\n",
    "              \n",
    "    def forward(self, input):\n",
    "        print(\"Applying module {} with input = {}\".format(self.name, input))\n",
    "        self.save_for_backward = input\n",
    "        output = torch.matmul(input, self.weight.data.t())\n",
    "        if self.bias: \n",
    "            output += self.bias.data\n",
    "        return output\n",
    "              \n",
    "    def backward(self, grad_output):\n",
    "        input = self.save_for_backward\n",
    "        grad_input = torch.matmul(grad_output, self.weight.data)\n",
    "        grad_weight = torch.matmul(grad_output.t(), input)\n",
    "        self.weight.gradient += grad_weight\n",
    "        if self.bias: \n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "            self.bias.grad += grad_bias\n",
    "        return grad_input \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        gain = calculate_gain('linear')\n",
    "        stdv = gain / math.sqrt(self.in_features)\n",
    "        bound = math.sqrt(3.0) * stdv\n",
    "        self.weight.data.uniform_(-bound, bound)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self, name):\n",
    "        super(ReLU, self).__init__(name)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        print(\"Applying module {} with input = \".format(self.name, input))\n",
    "        self.save_for_backward = input\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        input = self.save_for_backward\n",
    "        grad_input = grad_output.copy()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self, name):\n",
    "        super(MSELoss, self).__init__(name)\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        assert(input.size() == target.size()), \"Input size different to target size.\"\n",
    "        se = (input - target)**2\n",
    "        return torch.mean(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gain(nonlinearity='relu'):\n",
    "    linear_fns = ['linear', 'conv1d']\n",
    "    if nonlinearity in linear_fns or nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    else:\n",
    "        raise ValueEroor(\"Specified non-linearity is not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, params, defauts):\n",
    "        self.defaults = defaults\n",
    "        \n",
    "        assert isinstance(params, Parameter), \"params argument should be a dict of Parameter\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f94f6479dd8>), ('bias', <__main__.Parameter object at 0x7f94f6479c18>)])\n",
      "\n",
      "MODULE.PARAM\n",
      "Arrived in leaf module\n",
      "<generator object Module.param_per_module at 0x7f9524441228>\n",
      "\n",
      "MODULE.__call__\n",
      "Applying module fc1 with input = tensor([[2., 4.]])\n",
      "module(x) =  tensor([[-2.7707,  2.4128,  2.6585, -0.1829]])\n"
     ]
    }
   ],
   "source": [
    "module = Linear('fc1', 2, 4)\n",
    "for p in module.param():\n",
    "    print(\"parameter p = \", p)\n",
    "\n",
    "print()\n",
    "print(\"MODULE.PARAM\")\n",
    "print(module.param())\n",
    "\n",
    "print()\n",
    "print(\"MODULE.__call__\")\n",
    "x = torch.Tensor([[2, 4]])\n",
    "print(\"module(x) = \", module(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    Linear('fc1', 2, 4),\n",
    "    Linear('fc2', 4, 1),\n",
    "    ReLU('relu1')\n",
    "    )\n",
    "\n",
    "print(\"MODULE.__call__\")\n",
    "model(x)\n",
    "\n",
    "print()\n",
    "print(\"MODULE.PARAM\")\n",
    "for key, module in model._children.items():\n",
    "    print(\"module.param() = \", module.param())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second project, do we first accumulate the gradient then afterwards calculate the derivate of the loss wrt \n",
    "to the input.  Or do it the other way around.\n",
    "They are usually unrelated computations. Think about the following scenario. You have a batch of inputs x_0 to x_9. \n",
    "And a single parameter a. Thus the forward pass for this module is s_i = a*x_i. For the backward pass we get as \n",
    "input dl/ds_i for all i and we need to compute dl/da and dl/dx_i . It is quite obvious that \n",
    "dl/da = sum x_i * dl/ds_i for all i. And dl/dx_i = dl/ds_i * a. The order in which one computes the two is irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "x = torch.tensor([[1, 2], [2, 1], [3, 4]]).type(torch.FloatTensor).requires_grad_()\n",
    "y = torch.tensor([1, 0.4, 3])\n",
    "#x = torch.tensor([[1., 2.]]).requires_grad_()\n",
    "#y = torch.tensor([1.])\n",
    "\n",
    "model = nn.Sequential(nn.Linear(2, 10), nn.ReLU())\n",
    "\n",
    "print(\"PRINTING PARAMETERS\")\n",
    "for p in model.parameters():\n",
    "    print(\"p = \", p)\n",
    "y_pred = model(x)\n",
    "\n",
    "print(\"PRINTING PREDICTION\")\n",
    "print(\"y_pred = \", y_pred)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "loss = criterion(y_pred, y)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"PRINTING GRADIENT\")\n",
    "#print(\"loss.grad = \", autograd.grad(loss, x))\n",
    "loss.backward()\n",
    "for p in model.parameters():\n",
    "    print(\"p.grad = \", p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
