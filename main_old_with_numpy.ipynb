{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "#import torch\n",
    "#from torch import Tensor\n",
    "#torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\" Base class \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self._parameters = OrderedDict()\n",
    "        self._children = OrderedDict()\n",
    "        self.training = True\n",
    "        \n",
    "    def __call__(self, *input, **kwargs):\n",
    "        return self.forward(*input, **kwargs)\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *grad_output):\n",
    "        \"\"\" backward receives as input a pointer to a tensor or a tuple of tensors containing\n",
    "        the gradient of the loss (or the function of interest) wrt the module's output, accumulates\n",
    "        the gradient wrt the parameters, and returns a tensor or a tuple of tensors containing the \n",
    "        gradient of the loss wrt the module's input (Application of the chain rule)\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_children(self, module):\n",
    "        print(\"adding child = \", module)\n",
    "        assert isinstance(module, Module) and module is not None, \"Not a Module.\"\n",
    "        assert module.name not in self._children, \"Module {} already exists\".format(module.name)\n",
    "        self._children[module.name] = module\n",
    "        \n",
    "    def add_parameter(self, name, param):\n",
    "        assert isinstance(param, Parameter), \"Not a Parameter.\"\n",
    "        assert name not in self._parameters, \"Parameter {} already exists\".format(name)\n",
    "        self._parameters[name] = param\n",
    "        \n",
    "    def param(self, recurse=True):\n",
    "        \"\"\" param returns a dict of Parameters, each composed of a parameter tensor, \n",
    "        and a gradient tensor of same size. This list is empty for parameterless modules. \"\"\"\n",
    "        if recurse == False or self._children is not None:\n",
    "            print(\"Arrived in leaf module\")\n",
    "            return self.param_per_module()\n",
    "        #else:\n",
    "        #    for key_mod, module in self._children.items():\n",
    "        #        print(\"Looping over children, module = \", module)\n",
    "        #        for key_param, parameter in module._parameters:\n",
    "        #            return param_per_module()\n",
    "                    \n",
    "    \n",
    "    def param_per_module(self):\n",
    "        if self._parameters:\n",
    "            yield self._parameters\n",
    "        else:\n",
    "            yield None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__('NN')\n",
    "        for index, module in enumerate(args):\n",
    "            self.add_children(module)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        print(\"In Sequential.forward\")\n",
    "        self.save_for_backward = input\n",
    "        for key, module in self._children.items():\n",
    "            input = module(input)\n",
    "        return input\n",
    "    \n",
    "    def backward(self, *grad_ouput):\n",
    "        for key, module in self._children.items():\n",
    "            grad_output = module.backward(grad_output)\n",
    "        return grad_ouput\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Module may have tensor parameters, for each of which it should also have a \n",
    "# similar sized tensor gradient to accumulate the gradient during the backward pass\n",
    "class Parameter(object):\n",
    "    def __init__(self, tensor=None, grad=None, requires_grad=True):\n",
    "        #assert tensor is None or isinstance(tensor, torch.Tensor), \"Not a tensor\"\n",
    "        assert tensor is None or isinstance(tensor, np.ndarray), \"Not a tensor\"\n",
    "        self.data = tensor\n",
    "        #self.grad = torch.empty(tensor.size())\n",
    "        self.grad = np.empty(tensor.size)\n",
    "        self.requires_grad = requires_grad\n",
    "    \n",
    "    def set_data(self, tensor):\n",
    "        assert tensor is None or isinstance(tensor, np.ndarray), \"Not a tensor\"\n",
    "        self.data = tensor  \n",
    "    \n",
    "    def set_grad_zero(self):\n",
    "        print(\"setting grad of {} to zero\".format(self))\n",
    "        #self.grad = torch.zeros(self.grad.size())\n",
    "        print(\"self.grad.size = \", self.grad.size)\n",
    "        self.grad = np.zeros(self.grad.size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\" Implements a R^C -> R^D fully-connected layer:\n",
    "        Input: (N x C) tensor\n",
    "        Ouput: (N x D) tensor \"\"\"\n",
    "    def __init__(self, name, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__(name)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        #self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight = Parameter(np.empty((out_features, in_features)))\n",
    "        if bias:\n",
    "        #    self.bias = Parameter(torch.Tensor(out_features))\n",
    "            self.bias = Parameter(np.empty(out_features))\n",
    "        self.reset_parameters()\n",
    "        self.add_parameter('weight', self.weight)\n",
    "        self.add_parameter('bias', self.bias)\n",
    "              \n",
    "    def forward(self, input):\n",
    "        #print(\"Applying module {} with input = {}\".format(self.name, input))\n",
    "        self.save_for_backward = input\n",
    "        #output = torch.matmul(input, self.weight.data.t())\n",
    "        output = np.dot(input, self.weight.data.transpose())\n",
    "        if self.bias: \n",
    "            output += self.bias.data\n",
    "        return output\n",
    "              \n",
    "    def backward(self, grad_output):\n",
    "        input = self.save_for_backward\n",
    "        #grad_input = torch.matmul(grad_output, self.weight.data)\n",
    "        #grad_weight = torch.matmul(grad_output.t(), input)\n",
    "        grad_input = np.outer(grad_output.transpose(), self.weight.data)\n",
    "        grad_weight = np.dot(input.transpose(), grad_output.transpose())\n",
    "        self.weight.grad += grad_weight\n",
    "        if self.bias: \n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "            self.bias.grad += grad_bias\n",
    "        return grad_input \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        gain = calculate_gain('linear')\n",
    "        stdv = gain / math.sqrt(self.in_features)\n",
    "        bound = math.sqrt(3.0) * stdv\n",
    "        #self.weight.data.uniform_(-bound, bound)\n",
    "        #print(\"self.weight = \", self.weight)\n",
    "        self.weight.data = np.random.uniform(-bound, bound, self.weight.data.size)\n",
    "        if self.bias is not None:\n",
    "            #self.bias.data.uniform_(-bound, bound)\n",
    "            self.bias.data = np.random.uniform(-bound, bound, self.bias.data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self, name):\n",
    "        super(ReLU, self).__init__(name)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        print(\"Applying module {} with input = \".format(self.name, input))\n",
    "        self.save_for_backward = input\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        input = self.save_for_backward\n",
    "        grad_input = grad_output.copy()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self, name=None):\n",
    "        if name is None: name = 'mse'\n",
    "        super(MSELoss, self).__init__(name)\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        assert(input.size == target.size), \"Input size different to target size.\"\n",
    "        self.save_for_backward_input = input\n",
    "        self.save_for_backward_target = target\n",
    "        se = (input - target)**2\n",
    "        #return torch.mean(se)\n",
    "        return np.mean(se)\n",
    "    \n",
    "    def backward(self, grad_ouput=None):\n",
    "        input = self.save_for_backward_input\n",
    "        target = self.save_for_backward_target\n",
    "        grad_se = 2*(input - target) / len(input)\n",
    "        #return torch.mean(grad_se)\n",
    "        return grad_se\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gain(nonlinearity='relu'):\n",
    "    linear_fns = ['linear', 'conv1d']\n",
    "    if nonlinearity in linear_fns or nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    else:\n",
    "        raise ValueEroor(\"Specified non-linearity is not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, params, defaults):\n",
    "        self.defaults = defaults\n",
    "        #assert isinstance(params, Parameter) or isinstance(params, dict), (\"params argument should be a dict of Parameter, but is = {}\").format(params.type)\n",
    "        self.params = params\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        print(\"in zero_grad, self.params = \", self.params)\n",
    "        for p_group in self.params:\n",
    "            print(\"p_group = \", p_group)\n",
    "            for key, p in p_group.items():\n",
    "                print(\"p = \", p)\n",
    "                print(\"p.grad = \", p.grad)\n",
    "                print(\"p.grad.size = \", p.grad.size)\n",
    "                if p.grad is not None:\n",
    "                    p.set_grad_zero()\n",
    "                \n",
    "    def step(self, closure):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr=0.001):\n",
    "        defaults = dict(lr=lr)\n",
    "        super(SGD, self).__init__(params, defaults)\n",
    "        \n",
    "    def step(self, closure):\n",
    "        loss= None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "    \n",
    "        for p in self.params:\n",
    "            print(\"p = \", p)\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            d_p = p.grad\n",
    "            print(\"d_p = \", d_p)\n",
    "            p.data -= lr*d_p\n",
    "        \n",
    "        return loss       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f0311208>), ('bias', <__main__.Parameter object at 0x7f03f0311400>)])\n",
      "data =  [ 0.02483379 -0.9824095   0.90153609]\n",
      "data =  [0.50886631]\n"
     ]
    }
   ],
   "source": [
    "model = Linear('fc1', 3, 1)\n",
    "for p in model.param():\n",
    "    print(\"parameter p = \", p)\n",
    "    for key, param in p.items():\n",
    "        print(\"data = \", param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f02bc748>), ('bias', <__main__.Parameter object at 0x7f03f02bc7b8>)])\n",
      "data =  [-0.17404008  0.88734597 -0.42960771]\n",
      "data =  [0.060524]\n",
      "\n",
      "MODULE.PARAM\n",
      "Arrived in leaf module\n",
      "<generator object Module.param_per_module at 0x7f03f031d390>\n",
      "\n",
      "MODULE.__call__\n",
      "output =  [0.68418149 0.85822157]\n",
      "Arrived in leaf module\n",
      "\n",
      "\n",
      "new epoch, e =  0\n",
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f02bc748>), ('bias', <__main__.Parameter object at 0x7f03f02bc7b8>)])\n",
      "data =  [-0.17404008  0.88734597 -0.42960771]\n",
      "data =  [0.060524]\n",
      "\n",
      "calling zero_grad\n",
      "in zero_grad, self.params =  <generator object Module.param_per_module at 0x7f03f031d390>\n",
      "p_group =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f02bc748>), ('bias', <__main__.Parameter object at 0x7f03f02bc7b8>)])\n",
      "p =  <__main__.Parameter object at 0x7f03f02bc748>\n",
      "p.grad =  [ 0.02483379 -0.9824095   0.90153609]\n",
      "p.grad.size =  3\n",
      "setting grad of <__main__.Parameter object at 0x7f03f02bc748> to zero\n",
      "self.grad.size =  3\n",
      "p =  <__main__.Parameter object at 0x7f03f02bc7b8>\n",
      "p.grad =  [0.50886631]\n",
      "p.grad.size =  1\n",
      "setting grad of <__main__.Parameter object at 0x7f03f02bc7b8> to zero\n",
      "self.grad.size =  1\n",
      "\n",
      "loss =  0.8757397401739145\n",
      "\n",
      "new epoch, e =  1\n",
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f02bc748>), ('bias', <__main__.Parameter object at 0x7f03f02bc7b8>)])\n",
      "data =  [-0.17404008  0.88734597 -0.42960771]\n",
      "data =  [0.060524]\n",
      "\n",
      "calling zero_grad\n",
      "in zero_grad, self.params =  <generator object Module.param_per_module at 0x7f03f031d390>\n",
      "\n",
      "loss =  0.8757397401739145\n",
      "\n",
      "new epoch, e =  2\n",
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f02bc748>), ('bias', <__main__.Parameter object at 0x7f03f02bc7b8>)])\n",
      "data =  [-0.17404008  0.88734597 -0.42960771]\n",
      "data =  [0.060524]\n",
      "\n",
      "calling zero_grad\n",
      "in zero_grad, self.params =  <generator object Module.param_per_module at 0x7f03f031d390>\n",
      "\n",
      "loss =  0.8757397401739145\n",
      "\n",
      "new epoch, e =  3\n",
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f02bc748>), ('bias', <__main__.Parameter object at 0x7f03f02bc7b8>)])\n",
      "data =  [-0.17404008  0.88734597 -0.42960771]\n",
      "data =  [0.060524]\n",
      "\n",
      "calling zero_grad\n",
      "in zero_grad, self.params =  <generator object Module.param_per_module at 0x7f03f031d390>\n",
      "\n",
      "loss =  0.8757397401739145\n",
      "\n",
      "new epoch, e =  4\n",
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f02bc748>), ('bias', <__main__.Parameter object at 0x7f03f02bc7b8>)])\n",
      "data =  [-0.17404008  0.88734597 -0.42960771]\n",
      "data =  [0.060524]\n",
      "\n",
      "calling zero_grad\n",
      "in zero_grad, self.params =  <generator object Module.param_per_module at 0x7f03f031d390>\n",
      "\n",
      "loss =  0.8757397401739145\n",
      "\n",
      "new epoch, e =  5\n",
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f02bc748>), ('bias', <__main__.Parameter object at 0x7f03f02bc7b8>)])\n",
      "data =  [-0.17404008  0.88734597 -0.42960771]\n",
      "data =  [0.060524]\n",
      "\n",
      "calling zero_grad\n",
      "in zero_grad, self.params =  <generator object Module.param_per_module at 0x7f03f031d390>\n",
      "\n",
      "loss =  0.8757397401739145\n",
      "\n",
      "new epoch, e =  6\n",
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f02bc748>), ('bias', <__main__.Parameter object at 0x7f03f02bc7b8>)])\n",
      "data =  [-0.17404008  0.88734597 -0.42960771]\n",
      "data =  [0.060524]\n",
      "\n",
      "calling zero_grad\n",
      "in zero_grad, self.params =  <generator object Module.param_per_module at 0x7f03f031d390>\n",
      "\n",
      "loss =  0.8757397401739145\n",
      "\n",
      "new epoch, e =  7\n",
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f02bc748>), ('bias', <__main__.Parameter object at 0x7f03f02bc7b8>)])\n",
      "data =  [-0.17404008  0.88734597 -0.42960771]\n",
      "data =  [0.060524]\n",
      "\n",
      "calling zero_grad\n",
      "in zero_grad, self.params =  <generator object Module.param_per_module at 0x7f03f031d390>\n",
      "\n",
      "loss =  0.8757397401739145\n",
      "\n",
      "new epoch, e =  8\n",
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f02bc748>), ('bias', <__main__.Parameter object at 0x7f03f02bc7b8>)])\n",
      "data =  [-0.17404008  0.88734597 -0.42960771]\n",
      "data =  [0.060524]\n",
      "\n",
      "calling zero_grad\n",
      "in zero_grad, self.params =  <generator object Module.param_per_module at 0x7f03f031d390>\n",
      "\n",
      "loss =  0.8757397401739145\n",
      "\n",
      "new epoch, e =  9\n",
      "Arrived in leaf module\n",
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f03f02bc748>), ('bias', <__main__.Parameter object at 0x7f03f02bc7b8>)])\n",
      "data =  [-0.17404008  0.88734597 -0.42960771]\n",
      "data =  [0.060524]\n",
      "\n",
      "calling zero_grad\n",
      "in zero_grad, self.params =  <generator object Module.param_per_module at 0x7f03f031d390>\n",
      "\n",
      "loss =  0.8757397401739145\n",
      "\n",
      "\n",
      "grad_output =  [-1.31581851 -0.14177843]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.22900516, -1.16758626,  0.56528577],\n",
       "       [ 0.02467513, -0.12580652,  0.06090911]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Linear('fc1', 3, 1)\n",
    "for p in model.param():\n",
    "    print(\"parameter p = \", p)\n",
    "    for key, param in p.items():\n",
    "        print(\"data = \", param.data)\n",
    "\n",
    "print()\n",
    "print(\"MODULE.PARAM\")\n",
    "print(model.param())\n",
    "\n",
    "print()\n",
    "print(\"MODULE.__call__\")\n",
    "#input = torch.Tensor([[2, 4, 6], [1, 4, 6]])\n",
    "input = np.array([[2, 4, 6], [1, 4, 6]])\n",
    "output = model(input)\n",
    "print(\"output = \", output)\n",
    "#target = torch.Tensor([[2], [1]])\n",
    "target = np.array([2, 1])\n",
    "\n",
    "criterion = MSELoss()\n",
    "optimizer = SGD(model.param())\n",
    "\n",
    "#torch.set_grad_enabled(False)\n",
    "print()\n",
    "nb_epochs = 10\n",
    "for e in range(nb_epochs):\n",
    "    print()\n",
    "    print(\"new epoch, e = \", e)\n",
    "    for p in model.param():\n",
    "        print(\"parameter p = \", p)\n",
    "        for key, param in p.items():\n",
    "            print(\"data = \", param.data)\n",
    "            \n",
    "    print()\n",
    "    print(\"calling zero_grad\")\n",
    "    optimizer.zero_grad()\n",
    "    print()\n",
    "    \n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "    print(\"loss = \", loss)\n",
    "    grad_output = criterion.backward()\n",
    "    model.backward(grad_output)\n",
    "    optimizer.step(criterion(output, target))\n",
    "\n",
    "print()\n",
    "print()\n",
    "grad_output = criterion.backward()\n",
    "print(\"grad_output = \", grad_output)\n",
    "model.backward(grad_output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding child =  <__main__.Linear object at 0x7f03f02ae240>\n",
      "adding child =  <__main__.Linear object at 0x7f03f02ae320>\n",
      "adding child =  <__main__.ReLU object at 0x7f03f02ae2b0>\n",
      "MODULE.__call__\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5a647502a2b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MODULE.__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    Linear('fc1', 2, 4),\n",
    "    Linear('fc2', 4, 1),\n",
    "    ReLU('relu1')\n",
    "    )\n",
    "\n",
    "print(\"MODULE.__call__\")\n",
    "model(x)\n",
    "\n",
    "print()\n",
    "print(\"MODULE.PARAM\")\n",
    "for key, module in model._children.items():\n",
    "    print(\"module.param() = \", module.param())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second project, do we first accumulate the gradient then afterwards calculate the derivate of the loss wrt \n",
    "to the input.  Or do it the other way around.\n",
    "They are usually unrelated computations. Think about the following scenario. You have a batch of inputs x_0 to x_9. \n",
    "And a single parameter a. Thus the forward pass for this module is s_i = a*x_i. For the backward pass we get as \n",
    "input dl/ds_i for all i and we need to compute dl/da and dl/dx_i . It is quite obvious that \n",
    "dl/da = sum x_i * dl/ds_i for all i. And dl/dx_i = dl/ds_i * a. The order in which one computes the two is irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "x = torch.tensor([[1, 2], [2, 1], [3, 4]]).type(torch.FloatTensor).requires_grad_()\n",
    "y = torch.tensor([1, 0.4, 3])\n",
    "#x = torch.tensor([[1., 2.]]).requires_grad_()\n",
    "#y = torch.tensor([1.])\n",
    "\n",
    "model = nn.Sequential(nn.Linear(2, 10), nn.ReLU())\n",
    "\n",
    "print(\"PRINTING PARAMETERS\")\n",
    "for p in model.parameters():\n",
    "    print(\"p = \", p)\n",
    "y_pred = model(x)\n",
    "\n",
    "print(\"PRINTING PREDICTION\")\n",
    "print(\"y_pred = \", y_pred)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "loss = criterion(y_pred, y)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"PRINTING GRADIENT\")\n",
    "#print(\"loss.grad = \", autograd.grad(loss, x))\n",
    "loss.backward()\n",
    "for p in model.parameters():\n",
    "    print(\"p.grad = \", p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
