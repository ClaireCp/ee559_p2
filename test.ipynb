{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dlc_practical_prologue as prologue\n",
    "from importlib import reload\n",
    "reload(prologue)\n",
    "from dlc_practical_prologue import *\n",
    "import Module, modules, optimizers, helpers\n",
    "reload(Module)\n",
    "reload(modules)\n",
    "reload(helpers)\n",
    "from modules import *\n",
    "from helpers import *\n",
    "from optimizers import *\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = \\\n",
    "       load_random_datasets()\n",
    "\n",
    "input = torch.Tensor([[2., 4., 6.], [1., 4., 6.], [0.5, 3., 2.], [4.3, 4., 5.],\n",
    "                      [1.3, 4.1, 6.4], [1.4, 4.1, 6.5], [5, 4., 6.]])\n",
    "target = torch.Tensor([[2], [1], [1.1], [3], [1.2], [1.21], [3.4]])\n",
    "nb_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone Linear Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4593, -0.5606, -0.0493]])\n",
      "Parameter containing:\n",
      "tensor([0.8749])\n"
     ]
    }
   ],
   "source": [
    "model_lin = Linear('fc1', 3, 1)\n",
    "print_parameters_as_torch(model_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4593, -0.5606, -0.0493]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.8749], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "torch.set_grad_enabled(True)\n",
    "model_lin_torch = nn.Linear(3, 1)\n",
    "set_initial_parameters(model_lin, model_lin_torch)\n",
    "for param in model_lin_torch.parameters():\n",
    "    print(param)\n",
    "del nn\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def hook(module, grad_input, grad_output):\n",
    "    for grad in grad_output:\n",
    "        print(\"grad_output = \", grad_output[0].t())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = 0, loss = 22.67716, grad_output = tensor([[-1.3090, -0.8920, -0.6385, -1.8825, -1.0102, -1.0276, -2.1027]])\n",
      "e = 50, loss = 0.06753, grad_output = tensor([[-0.0686,  0.0748, -0.1039, -0.0644,  0.0683,  0.0834, -0.0416]])\n",
      "e = 100, loss = 0.03252, grad_output = tensor([[-0.0833,  0.0376, -0.0835, -0.0064,  0.0309,  0.0462,  0.0113]])\n",
      "e = 150, loss = 0.02522, grad_output = tensor([[-0.0865,  0.0326, -0.0606,  0.0047,  0.0236,  0.0380,  0.0134]])\n",
      "e = 200, loss = 0.02171, grad_output = tensor([[-0.0880,  0.0314, -0.0434,  0.0093,  0.0205,  0.0342,  0.0110]])\n",
      "e = 250, loss = 0.01997, grad_output = tensor([[-0.0890,  0.0308, -0.0312,  0.0121,  0.0185,  0.0317,  0.0089]])\n",
      "\n",
      "e = 0, loss = 22.67716\n",
      "e = 50, loss = 0.06753\n",
      "e = 100, loss = 0.03252\n",
      "e = 150, loss = 0.02522\n",
      "e = 200, loss = 0.02171\n",
      "e = 250, loss = 0.01997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f4bd8113fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_lin = MSELoss()\n",
    "optimizer_lin = SGD(model_lin, lr=0.01)\n",
    "nb_epochs = 300\n",
    "for e in range(nb_epochs):\n",
    "    optimizer_lin.zero_grad()\n",
    "    output_lin = model_lin(input)\n",
    "    loss_lin = criterion_lin(output_lin, target)\n",
    "    grad_output_lin = criterion_lin.backward()\n",
    "    if e % 50 == 0: print(\"e = {}, loss = {}, grad_output = {}\".format(e, round(loss_lin.item(), 5), grad_output_lin.t()))\n",
    "    model_lin.backward(grad_output_lin)\n",
    "    optimizer_lin.step(loss_lin)\n",
    " \n",
    "print()\n",
    "from torch import nn\n",
    "torch.set_grad_enabled(True)\n",
    "    \n",
    "#model_lin_torch.register_backward_hook(hook)     \n",
    "criterion_lin_torch = nn.MSELoss()\n",
    "optimizer_lin_torch = torch.optim.SGD(model_lin_torch.parameters(), lr=0.01)\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    optimizer_lin_torch.zero_grad()\n",
    "    output_lin_torch = model_lin_torch(input)\n",
    "    loss_lin_torch = criterion_lin_torch(output_lin_torch, target)\n",
    "    if e % 50 == 0: print(\"e = {}, loss = {}\".format(e, round(loss_lin_torch.item(), 5)))\n",
    "    loss_lin_torch.backward()\n",
    "    optimizer_lin_torch.step()\n",
    "    \n",
    "del nn\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5055, -0.5256, -0.5978],\n",
      "        [ 0.3847, -0.6245, -0.4432],\n",
      "        [-0.5773, -0.9464,  0.2816],\n",
      "        [ 0.9112, -0.0370, -0.0581],\n",
      "        [ 0.6176, -0.6392,  0.8895],\n",
      "        [-0.9584, -0.9219, -0.6092]])\n",
      "Parameter containing:\n",
      "tensor([-0.1793, -0.0232, -0.4167, -0.0853,  0.2761, -0.3865])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0910,  0.5517, -0.0885, -0.4841, -0.0721,  0.1615]])\n",
      "Parameter containing:\n",
      "tensor([0.6264])\n"
     ]
    }
   ],
   "source": [
    "model_seq = Sequential(\n",
    "    Linear('fc1', 3, 6), \n",
    "    ReLU('relu'),\n",
    "    Linear('fc2', 6, 1), \n",
    "    Tanh())\n",
    "print_parameters_as_torch(model_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5055, -0.5256, -0.5978],\n",
      "        [ 0.3847, -0.6245, -0.4432],\n",
      "        [-0.5773, -0.9464,  0.2816],\n",
      "        [ 0.9112, -0.0370, -0.0581],\n",
      "        [ 0.6176, -0.6392,  0.8895],\n",
      "        [-0.9584, -0.9219, -0.6092]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1793, -0.0232, -0.4167, -0.0853,  0.2761, -0.3865],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0910,  0.5517, -0.0885, -0.4841, -0.0721,  0.1615]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6264], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import Module, modules, helpers\n",
    "reload(Module)\n",
    "reload(modules)\n",
    "reload(helpers)\n",
    "from modules import *\n",
    "from helpers import *\n",
    "\n",
    "from torch import nn\n",
    "model_seq_torch = nn.Sequential(\n",
    "    nn.Linear(3, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 1),\n",
    "    nn.Tanh())\n",
    "set_initial_parameters(model_seq, model_seq_torch)\n",
    "for param in model_seq_torch.parameters():\n",
    "    print(param)\n",
    "del nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = 0, loss = 6.12547, grad_output = tensor([[-0.6504, -0.2288, -0.1767, -1.1079, -0.3291, -0.3468, -1.2401]])\n",
      "e = 50, loss = 1.57065, grad_output = tensor([[-0.2874, -0.0029, -0.0852, -0.5737, -0.0590, -0.0616, -0.6865]])\n",
      "e = 100, loss = 1.56224, grad_output = tensor([[-0.2865, -0.0014, -0.0737, -0.5723, -0.0580, -0.0607, -0.6860]])\n",
      "e = 150, loss = 1.55903, grad_output = tensor([[-0.2862, -0.0009, -0.0669, -0.5719, -0.0577, -0.0604, -0.6859]])\n",
      "e = 200, loss = 1.55727, grad_output = tensor([[-2.8603e-01, -6.3724e-04, -6.2249e-02, -5.7177e-01, -5.7499e-02,\n",
      "         -6.0299e-02, -6.8580e-01]])\n",
      "e = 250, loss = 1.55614, grad_output = tensor([[-2.8594e-01, -4.7685e-04, -5.8754e-02, -5.7167e-01, -5.7403e-02,\n",
      "         -6.0217e-02, -6.8577e-01]])\n",
      "\n",
      "e = 0, loss = 6.12547\n",
      "e = 50, loss = 1.57065\n",
      "e = 100, loss = 1.56224\n",
      "e = 150, loss = 1.55903\n",
      "e = 200, loss = 1.55727\n",
      "e = 250, loss = 1.55615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f4bd80d54e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_seq = MSELoss()\n",
    "optimizer_seq = SGD(model_seq, lr=0.01)\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    optimizer_seq.zero_grad()\n",
    "    output_seq = model_seq(input)\n",
    "    loss_seq = criterion_seq(output_seq, target)\n",
    "    grad_output_seq = criterion_seq.backward()\n",
    "    #print(grad_output_seq.shape)\n",
    "    if e % 50 == 0: print(\"e = {}, loss = {}, grad_output = {}\".format(e, round(loss_seq.item(), 5), grad_output_seq.t()))\n",
    "    model_seq.backward(grad_output_seq)\n",
    "    optimizer_seq.step(loss_seq)\n",
    "\n",
    "print()\n",
    "from torch import nn\n",
    "torch.set_grad_enabled(True)\n",
    "    \n",
    "#model_seq_torch.register_backward_hook(hook)     \n",
    "criterion_seq_torch = nn.MSELoss()\n",
    "optimizer_seq_torch = torch.optim.SGD(model_seq_torch.parameters(), lr=0.01)\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    optimizer_seq_torch.zero_grad()\n",
    "    output_seq_torch = model_seq_torch(input)\n",
    "    loss_seq_torch = criterion_seq_torch(output_seq_torch, target)\n",
    "    if e % 50 == 0: print(\"e = {}, loss = {}\".format(e, round(loss_seq_torch.item(), 5)))\n",
    "    loss_seq_torch.backward()\n",
    "    optimizer_seq_torch.step()\n",
    "    \n",
    "del nn\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Module with multiple sigmoid and Binary-Cross-Entropy Loss\n",
    "Idea: check behavior of sigmoid and with multiple parameterless (same) functions without unique names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0871, -0.2809, -0.6147],\n",
      "        [ 0.8904,  0.9812, -0.3240],\n",
      "        [-0.4314,  0.6022,  0.1325],\n",
      "        [ 0.4467, -0.3703,  0.1463],\n",
      "        [ 0.5268,  0.7447,  0.5614],\n",
      "        [ 0.9037,  0.5998,  0.1722]])\n",
      "Parameter containing:\n",
      "tensor([-0.0836, -0.8084,  0.2817, -0.0101,  0.4545,  0.9739])\n",
      "Parameter containing:\n",
      "tensor([[ 0.4564,  0.0860,  0.0347, -0.0512, -0.3506,  0.2918]])\n",
      "Parameter containing:\n",
      "tensor([-0.5381])\n"
     ]
    }
   ],
   "source": [
    "model_seq_sigmoid = Sequential(\n",
    "    Linear('fc1', 3, 6), \n",
    "    Sigmoid('sig1'),\n",
    "    Linear('fc2', 6, 1), \n",
    "    Sigmoid('sig2'))\n",
    "print_parameters_as_torch(model_seq_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0871, -0.2809, -0.6147],\n",
      "        [ 0.8904,  0.9812, -0.3240],\n",
      "        [-0.4314,  0.6022,  0.1325],\n",
      "        [ 0.4467, -0.3703,  0.1463],\n",
      "        [ 0.5268,  0.7447,  0.5614],\n",
      "        [ 0.9037,  0.5998,  0.1722]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0836, -0.8084,  0.2817, -0.0101,  0.4545,  0.9739],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4564,  0.0860,  0.0347, -0.0512, -0.3506,  0.2918]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5381], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import Module, modules, helpers\n",
    "reload(Module)\n",
    "reload(modules)\n",
    "reload(helpers)\n",
    "from modules import *\n",
    "from helpers import *\n",
    "\n",
    "from torch import nn\n",
    "model_seq_sigmoid_torch = nn.Sequential(\n",
    "    nn.Linear(3, 6),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(6, 1),\n",
    "    nn.Sigmoid())\n",
    "set_initial_parameters(model_seq_sigmoid, model_seq_sigmoid_torch)\n",
    "for param in model_seq_sigmoid_torch.parameters():\n",
    "    print(param)\n",
    "del nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = 0, loss = 2.99611, grad_output = tensor([[-0.4641, -0.1784, -0.2038, -0.7501, -0.2356, -0.2385, -0.8650]])\n",
      "e = 50, loss = 2.06077, grad_output = tensor([[-0.3610, -0.0773, -0.1063, -0.6454, -0.1336, -0.1363, -0.7596]])\n",
      "e = 100, loss = 1.78921, grad_output = tensor([[-0.3239, -0.0401, -0.0703, -0.6083, -0.0964, -0.0991, -0.7223]])\n",
      "e = 150, loss = 1.6985, grad_output = tensor([[-0.3102, -0.0260, -0.0565, -0.5949, -0.0825, -0.0852, -0.7089]])\n",
      "e = 200, loss = 1.6562, grad_output = tensor([[-0.3035, -0.0190, -0.0495, -0.5884, -0.0756, -0.0784, -0.7024]])\n",
      "e = 250, loss = 1.63224, grad_output = tensor([[-0.2996, -0.0149, -0.0453, -0.5846, -0.0716, -0.0744, -0.6987]])\n",
      "\n",
      "e = 0, loss = 2.99611\n",
      "e = 50, loss = 2.06077\n",
      "e = 100, loss = 1.78921\n",
      "e = 150, loss = 1.6985\n",
      "e = 200, loss = 1.6562\n",
      "e = 250, loss = 1.63224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f4bd812c320>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_seq_sigmoid = MSELoss()\n",
    "optimizer_seq_sigmoid = SGD(model_seq_sigmoid, lr=0.01)\n",
    "\n",
    "nb_epochs = 300\n",
    "for e in range(nb_epochs):\n",
    "    optimizer_seq_sigmoid.zero_grad()\n",
    "    output_seq_sigmoid = model_seq_sigmoid(input)\n",
    "    loss_seq_sigmoid = criterion_seq_sigmoid(output_seq_sigmoid, target)\n",
    "    grad_output_seq_sigmoid = criterion_seq_sigmoid.backward()\n",
    "    if e % 50 == 0: print(\"e = {}, loss = {}, grad_output = {}\".format(e, round(loss_seq_sigmoid.item(), 5), grad_output_seq_sigmoid.t()))\n",
    "    model_seq_sigmoid.backward(grad_output_seq_sigmoid)\n",
    "    optimizer_seq_sigmoid.step(loss_seq_sigmoid)\n",
    "\n",
    "print()\n",
    "from torch import nn\n",
    "torch.set_grad_enabled(True)\n",
    "        \n",
    "criterion_seq_sigmoid_torch = nn.MSELoss()\n",
    "optimizer_seq_sigmoid_torch = torch.optim.SGD(model_seq_sigmoid_torch.parameters(), lr=0.01)\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    optimizer_seq_sigmoid_torch.zero_grad()\n",
    "    output_seq_sigmoid_torch = model_seq_sigmoid_torch(input)\n",
    "    loss_seq_sigmoid_torch = criterion_seq_sigmoid_torch(output_seq_sigmoid_torch, target)\n",
    "    if e % 50 == 0: print(\"e = {}, loss = {}\".format(e, round(loss_seq_sigmoid_torch.item(), 5)))\n",
    "    loss_seq_sigmoid_torch.backward()\n",
    "    optimizer_seq_sigmoid_torch.step()\n",
    "    \n",
    "del nn\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = 0, loss = 0.73909, grad_output = tensor([[ 2.5906, -1.6433,  2.3305,  2.7748,  2.5608, -1.6392, -1.5278]])\n",
      "e = 50, loss = 0.67347, grad_output = tensor([[ 1.8130, -2.2114,  1.7695,  1.8540,  1.8180, -2.2246, -2.0967]])\n",
      "e = 100, loss = 0.66557, grad_output = tensor([[ 1.7970, -2.2456,  1.7025,  1.8371,  1.8018, -2.2459, -2.0662]])\n",
      "e = 150, loss = 0.65823, grad_output = tensor([[ 1.8008, -2.2465,  1.6531,  1.8230,  1.8090, -2.2302, -2.0379]])\n",
      "e = 200, loss = 0.65163, grad_output = tensor([[ 1.8104, -2.2312,  1.6115,  1.8033,  1.8260, -2.2013, -2.0288]])\n",
      "e = 250, loss = 0.645, grad_output = tensor([[ 1.8223, -2.2068,  1.5718,  1.7795,  1.8486, -2.1666, -2.0284]])\n",
      "\n",
      "e = 0, loss = 0.73909\n",
      "e = 50, loss = 0.70213\n",
      "e = 100, loss = 0.68734\n",
      "e = 150, loss = 0.68095\n",
      "e = 200, loss = 0.67784\n",
      "e = 250, loss = 0.676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f4bd80eada0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Module, modules, helpers\n",
    "reload(Module)\n",
    "reload(modules)\n",
    "reload(helpers)\n",
    "from modules import *\n",
    "from helpers import *\n",
    "\n",
    "target_bce = torch.Tensor([0, 1, 0, 0, 0, 1, 1])\n",
    "\n",
    "model_seq_bce = Sequential(\n",
    "    Linear('fc1', 3, 6), \n",
    "    Sigmoid('sig1'),\n",
    "    Linear('fc2', 6, 1), \n",
    "    Sigmoid('sig2'))\n",
    "\n",
    "from torch import nn\n",
    "model_seq_bce_torch = nn.Sequential(\n",
    "    nn.Linear(3, 6),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(6, 1),\n",
    "    nn.Sigmoid())\n",
    "set_initial_parameters(model_seq_bce, model_seq_bce_torch)\n",
    "del nn\n",
    "\n",
    "criterion_seq_bce = BCELoss()\n",
    "optimizer_seq_bce = SGD(model_seq_bce, lr=0.01)\n",
    "\n",
    "nb_epochs = 300\n",
    "for e in range(nb_epochs):\n",
    "    optimizer_seq_bce.zero_grad()\n",
    "    output_seq_bce = model_seq_bce(input).view(-1)\n",
    "    loss_seq_bce = criterion_seq_bce(input=output_seq_bce, target=target_bce)\n",
    "    grad_output_seq_bce = criterion_seq_bce.backward().view((-1, 1))\n",
    "    if e % 50 == 0: print(\"e = {}, loss = {}, grad_output = {}\".format(e, round(loss_seq_bce.item(), 5), grad_output_seq_bce.t()))\n",
    "    model_seq_bce.backward(grad_output_seq_bce)\n",
    "    optimizer_seq_bce.step(loss_seq_bce)\n",
    "\n",
    "print()\n",
    "from torch import nn\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "#model_seq_bce_torch.register_backward_hook(hook)\n",
    "criterion_seq_bce_torch = nn.BCELoss()\n",
    "optimizer_seq_bce_torch = torch.optim.SGD(model_seq_bce_torch.parameters(), lr=0.01)\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    optimizer_seq_bce_torch.zero_grad()\n",
    "    output_seq_bce_torch = model_seq_bce_torch(input)\n",
    "    loss_seq_bce_torch = criterion_seq_bce_torch(output_seq_bce_torch.view(-1), target_bce)\n",
    "    if e % 50 == 0: print(\"e = {}, loss = {}\".format(e, round(loss_seq_bce_torch.item(), 5)))\n",
    "    loss_seq_bce_torch.backward()\n",
    "    optimizer_seq_bce_torch.step()\n",
    "    \n",
    "del nn\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.9623, -0.3760, -0.2283]])\n",
      "Parameter containing:\n",
      "tensor([[-0.9623, -0.3760, -0.2283]], requires_grad=True)\n",
      "e = 0, loss = 2.29588, grad_output = tensor([[ 1.0082e+00, -4.7344e+01,  1.1267e+00,  1.0011e+00,  1.0142e+00,\n",
      "         -8.0261e+01, -2.1769e+03]])\n",
      "e = 50, loss = 1.94327, grad_output = tensor([[   1.0189,  -21.9541,    1.1826,    1.0026,    1.0331,  -34.6047,\n",
      "         -852.6951]])\n",
      "e = 100, loss = 1.61959, grad_output = tensor([[   1.0418,  -10.8043,    1.2588,    1.0057,    1.0744,  -15.7804,\n",
      "         -347.1766]])\n",
      "e = 150, loss = 1.34406, grad_output = tensor([[   1.0870,   -5.8747,    1.3565,    1.0118,    1.1567,   -7.9388,\n",
      "         -151.4608]])\n",
      "e = 200, loss = 1.13462, grad_output = tensor([[  1.1645,  -3.6603,   1.4703,   1.0225,   1.2990,  -4.6000, -73.4927]])\n",
      "e = 250, loss = 0.99465, grad_output = tensor([[  1.2760,  -2.6305,   1.5877,   1.0380,   1.5045,  -3.1158, -40.7334]])\n",
      "p.grad =  tensor([[-4.5885, -4.0196, -7.2057]])\n",
      "\n",
      "e = 0, loss = 2.29588\n",
      "e = 50, loss = 2.24432\n",
      "e = 100, loss = 2.1931\n",
      "e = 150, loss = 2.14226\n",
      "e = 200, loss = 2.09182\n",
      "e = 250, loss = 2.04183\n",
      "p.grad =  tensor([[-1.0235, -1.5989, -2.4992]])\n"
     ]
    }
   ],
   "source": [
    "import Module, modules, helpers\n",
    "reload(Module)\n",
    "reload(modules)\n",
    "reload(helpers)\n",
    "from modules import *\n",
    "from helpers import *\n",
    "\n",
    "input = torch.Tensor([[2., 4., 6.], [1., 4., 6.], [0.5, 3., 2.], [4.3, 4., 5.],\n",
    "                      [1.3, 4.1, 6.4], [1.4, 4.1, 6.5], [5, 4., 6.]])\n",
    "target = torch.Tensor([[0], [1], [1], [0], [1], [1], [0]])\n",
    "model_seq_bce = Sequential(\n",
    "    Linear('fc1', 3, 1, bias=False),\n",
    "    Sigmoid())\n",
    "print_parameters_as_torch(model_seq_bce)\n",
    "\n",
    "from torch import nn\n",
    "model_seq_bce_torch = nn.Sequential(nn.Linear(3, 1, bias=False), nn.Sigmoid())\n",
    "set_initial_parameters(model_seq_bce, model_seq_bce_torch)\n",
    "for p in model_seq_bce_torch.parameters():\n",
    "    print(p)\n",
    "del nn\n",
    "\n",
    "criterion = BCELoss()\n",
    "optimizer_seq_bce = SGD(model_seq_bce, lr=0.0001)\n",
    "\n",
    "nb_epochs = 300\n",
    "for e in range(nb_epochs):\n",
    "    optimizer_seq_bce.zero_grad()\n",
    "    output_seq_bce = model_seq_bce(input).view(-1)\n",
    "    loss_seq_bce = criterion_seq_bce(input=output_seq_bce, target=target_bce)\n",
    "    grad_output_seq_bce = criterion_seq_bce.backward().view((-1, 1))\n",
    "    if e % 50 == 0: print(\"e = {}, loss = {}, grad_output = {}\".format(e, round(loss_seq_bce.item(), 5), grad_output_seq_bce.t()))\n",
    "    model_seq_bce.backward(grad_output_seq_bce)\n",
    "    optimizer_seq_bce.step(loss_seq_bce)\n",
    "\n",
    "for param_dict in model_seq_bce.param():\n",
    "    param_dict = next(param_dict)\n",
    "    for p in param_dict.values():\n",
    "        print(\"p.grad = \", p.grad.t())\n",
    "\n",
    "print()\n",
    "from torch import nn\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "criterion_torch = nn.BCELoss()\n",
    "optimizer_seq_bce_torch = torch.optim.SGD(model_seq_bce_torch.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    optimizer_seq_bce_torch.zero_grad()\n",
    "    output_seq_bce_torch = model_seq_bce_torch(input)\n",
    "    loss_seq_bce_torch = criterion_seq_bce_torch(output_seq_bce_torch.view(-1), target_bce)\n",
    "    if e % 50 == 0: print(\"e = {}, loss = {}\".format(e, round(loss_seq_bce_torch.item(), 5)))\n",
    "    loss_seq_bce_torch.backward()\n",
    "    optimizer_seq_bce_torch.step()\n",
    "    \n",
    "for p in model_seq_bce_torch.parameters():\n",
    "    print(\"p.grad = \", p.grad)\n",
    "\n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests on mnist-pairs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input.shape =  torch.Size([1000, 2, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  float is a single precision (32 bit) floating point data type, double is a double precision (64 bit) floating point\n",
    "nb_hidden1 = 100\n",
    "nb_hidden2 = 40\n",
    "print(\"train_input.shape = \", train_input.shape)\n",
    "train_input = train_input.view(len(train_input), -1)\n",
    "train_target = train_target.type(torch.FloatTensor)\n",
    "test_input = test_input.view(len(train_input), -1)\n",
    "test_target = test_target.type(torch.FloatTensor)\n",
    "input_size = train_input.shape[1] # 392\n",
    "nb_epochs = 300\n",
    "\n",
    "model_mnist = Sequential(\n",
    "    Linear('fc1', input_size, nb_hidden1), ReLU(),\n",
    "    Linear('fc2', nb_hidden1, nb_hidden2), Tanh(),\n",
    "    Linear('fc3', nb_hidden2, 1))\n",
    "\n",
    "from torch import nn\n",
    "model_mnist_torch = nn.Sequential(\n",
    "    nn.Linear(input_size, nb_hidden1), nn.ReLU(),\n",
    "    nn.Linear(nb_hidden1, nb_hidden2), nn.Tanh(),\n",
    "    nn.Linear(nb_hidden2, 1))\n",
    "set_initial_parameters(model_mnist, model_mnist_torch)\n",
    "del nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_input, test_target):\n",
    "    test_output = model(test_input)\n",
    "    output_to_prediction = torch.ge(test_output, 0.5).flatten()\n",
    "    nb_correct = torch.sum(output_to_prediction == test_target.type(torch.ByteTensor)).item()\n",
    "    acc_pairs = nb_correct / len(test_input)\n",
    "    return acc_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e = 0, test accuracy = 0.49, loss = 0.6887851357460022\n",
      "e = 40, test accuracy = 0.527, loss = 0.4702855348587036\n",
      "e = 80, test accuracy = 0.521, loss = 0.7507086396217346\n",
      "e = 120, test accuracy = 0.499, loss = 2.030029535293579\n",
      "e = 160, test accuracy = 0.515, loss = 2.0331625938415527\n",
      "e = 200, test accuracy = 0.49, loss = 10.023592948913574\n",
      "e = 240, test accuracy = 0.487, loss = 12.449797630310059\n",
      "e = 280, test accuracy = 0.483, loss = 1.1628178358078003\n",
      "\n",
      "e = 0, test accuracy = 0.49, loss = 0.6887850761413574\n",
      "e = 40, test accuracy = 0.684, loss = 0.19535726308822632\n",
      "e = 80, test accuracy = 0.716, loss = 0.15988074243068695\n",
      "e = 120, test accuracy = 0.735, loss = 0.1407870054244995\n",
      "e = 160, test accuracy = 0.742, loss = 0.12752914428710938\n",
      "e = 200, test accuracy = 0.753, loss = 0.1173262894153595\n",
      "e = 240, test accuracy = 0.75, loss = 0.10893375426530838\n",
      "e = 280, test accuracy = 0.752, loss = 0.10165887326002121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f4bd809f6a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Module, modules, helpers\n",
    "reload(Module)\n",
    "reload(modules)\n",
    "reload(helpers)\n",
    "from modules import *\n",
    "from helpers import *\n",
    "\n",
    "\n",
    "criterion_mnist = MSELoss()\n",
    "optimizer_mnist = SGD(model_mnist, lr=0.01)\n",
    "loss_history_mnist = []\n",
    "test_acc_history_mnist = []\n",
    "nb_epochs = 300\n",
    "for e in range(nb_epochs):\n",
    "    optimizer_mnist.zero_grad()\n",
    "    output_mnist = model_mnist(train_input) # torch.Size([1000, 1])\n",
    "    loss_mnist = criterion_mnist(output_mnist.flatten(), train_target)\n",
    "    loss_history_mnist.append(loss_mnist)\n",
    "    grad_output_mnist = criterion_mnist.backward().view(-1,1)\n",
    "    test_acc_mnist = test_model(model_mnist, test_input, test_target)\n",
    "    test_acc_history_mnist.append(test_acc_mnist)\n",
    "    if e % 40 == 0: print(\"e = {}, test accuracy = {}, loss = {}\".format(e, test_acc_mnist, loss_mnist))\n",
    "    #print(\"e = {}, test accuracy = {}, loss = {}\".format(e, test_acc_mnist, loss_mnist))\n",
    "    model_mnist.backward(grad_output_mnist)\n",
    "    optimizer_mnist.step(loss_mnist)\n",
    "\n",
    "print()\n",
    "from torch import nn\n",
    "torch.set_grad_enabled(True)\n",
    "def hook(module, grad_input, grad_output):\n",
    "    #print(\"grad_input = \", grad_input[0][:5].t())\n",
    "    for grad in grad_output:\n",
    "        print(\"grad_output = \", grad_output[0][:5].t())\n",
    "        break\n",
    "        \n",
    "criterion_mnist_torch = nn.MSELoss()\n",
    "optimizer_mnist_torch = torch.optim.SGD(model_mnist_torch.parameters(), lr=0.01)\n",
    "#model_mnist_torch.register_backward_hook(hook)\n",
    "loss_history_mnist_torch = []\n",
    "test_acc_history_mnist_torch = []\n",
    "\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    optimizer_mnist_torch.zero_grad()\n",
    "    output_mnist_torch = model_mnist_torch(train_input)\n",
    "    #print(\"output = \", output_mnist_torch[:5])\n",
    "    #print()\n",
    "    loss_mnist_torch = criterion_mnist_torch(output_mnist_torch.flatten(), train_target)\n",
    "    loss_history_mnist_torch.append(loss_mnist_torch)\n",
    "    test_acc_mnist_torch = test_model(model_mnist_torch, test_input, test_target)\n",
    "    test_acc_history_mnist_torch.append(test_acc_mnist_torch)\n",
    "    if e % 40 == 0: print(\"e = {}, test accuracy = {}, loss = {}\".format(e, test_acc_mnist_torch, loss_mnist_torch))\n",
    "    #print(\"e = {}, test accuracy = {}, loss = {}\".format(e, test_acc_mnist_torch, loss_mnist_torch))\n",
    "    loss_mnist_torch.backward()\n",
    "    optimizer_mnist_torch.step()\n",
    "    \n",
    "del nn\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_history_mnist, label='Custom DL framework')\n",
    "plt.plot(loss_history_mnist_torch, label='PyTorch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(test_acc_history_mnist, label='Custom DL framework')\n",
    "plt.plot(test_acc_history_mnist_torch, label='PyTorch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
