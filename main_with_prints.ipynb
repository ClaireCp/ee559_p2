{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f782d7c4550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import Tensor\n",
    "torch.manual_seed(0)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\" Base class \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self._parameters = OrderedDict()\n",
    "        self._children = OrderedDict()\n",
    "        self.training = True\n",
    "        \n",
    "    def __call__(self, *input, **kwargs):\n",
    "        return self.forward(*input, **kwargs)\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *grad_output):\n",
    "        \"\"\" backward receives as input a pointer to a tensor or a tuple of tensors containing\n",
    "        the gradient of the loss (or the function of interest) wrt the module's output, accumulates\n",
    "        the gradient wrt the parameters, and returns a tensor or a tuple of tensors containing the \n",
    "        gradient of the loss wrt the module's input (Application of the chain rule)\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_children(self, module):\n",
    "        print(\"adding child = \", module)\n",
    "        assert isinstance(module, Module) and module is not None, \"Not a Module.\"\n",
    "        assert module.name not in self._children, \"Module {} already exists\".format(module.name)\n",
    "        self._children[module.name] = module\n",
    "        \n",
    "    def add_parameter(self, name, param):\n",
    "        assert isinstance(param, Parameter), \"Not a Parameter.\"\n",
    "        assert name not in self._parameters, \"Parameter {} already exists\".format(name)\n",
    "        self._parameters[name] = param\n",
    "        \n",
    "    def param(self, recurse=True):\n",
    "        \"\"\" param returns a dict of Parameters, each composed of a parameter tensor, \n",
    "        and a gradient tensor of same size. This list is empty for parameterless modules. \"\"\"\n",
    "        if recurse == False or self._children is not None:\n",
    "            #print(\"Arrived in leaf module\")\n",
    "            return self.param_per_module()\n",
    "        else:\n",
    "            for key_mod, module in self._children.items():\n",
    "                #print(\"Looping over children, module = \", module)\n",
    "                for key_param, parameter in module._parameters:\n",
    "                    return param_per_module()\n",
    "                    \n",
    "    \n",
    "    def param_per_module(self):\n",
    "        if self._parameters:\n",
    "            yield self._parameters\n",
    "        else:\n",
    "            yield None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__('NN')\n",
    "        for index, module in enumerate(args):\n",
    "            self.add_children(module)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        print(\"In Sequential.forward\")\n",
    "        self.save_for_backward = input\n",
    "        for key, module in self._children.items():\n",
    "            input = module(input)\n",
    "        return input\n",
    "    \n",
    "    def backward(self, *grad_output):\n",
    "        for key, module in self._children.items():\n",
    "            grad_output = module.backward(grad_output)\n",
    "        return grad_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Module may have tensor parameters, for each of which it should also have a \n",
    "# similar sized tensor gradient to accumulate the gradient during the backward pass\n",
    "class Parameter(object):\n",
    "    def __init__(self, tensor=None, grad=None, requires_grad=True):\n",
    "        assert tensor is None or isinstance(tensor, torch.Tensor), \"Not a tensor\"\n",
    "        self.data = tensor\n",
    "        self.grad = torch.empty(tensor.size())\n",
    "        self.requires_grad = requires_grad\n",
    "    \n",
    "    def set_data(self, tensor):\n",
    "        assert tensor is None or isinstance(tensor, torch.Tensor), \"Not a tensor\"\n",
    "        self.data = tensor  \n",
    "    \n",
    "    def set_grad_zero(self):\n",
    "        print(\"setting grad of {} to zero\".format(self))\n",
    "        self.grad = torch.zeros(self.grad.size())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\" Implements a R^C -> R^D fully-connected layer:\n",
    "        Input: (N x C) tensor\n",
    "        Ouput: (N x D) tensor \"\"\"\n",
    "    def __init__(self, name, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__(name)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        self.reset_parameters()\n",
    "        self.add_parameter('weight', self.weight)\n",
    "        self.add_parameter('bias', self.bias)\n",
    "              \n",
    "    def forward(self, input):\n",
    "        \n",
    "        print(\"Computing the forward pass of module {} \".format(self.name))\n",
    "        print(\"input = {}, input.size = {}\".format(input, input.size()))\n",
    "        print(\"weight = {}, weight.size = {}\".format(self.weight, self.weight.data.size()))\n",
    "        print(\"bias = {}, bias.size = {}\".format(self.bias, self.bias.data.size()))\n",
    "        \n",
    "        self.save_for_backward = input\n",
    "        output = torch.matmul(input, self.weight.data)\n",
    "        if self.bias: \n",
    "            output += self.bias.data\n",
    "            \n",
    "        print(\"output = {}, output.size = {}\".format(output, output.size))\n",
    "        return output\n",
    "              \n",
    "    def backward(self, grad_output):\n",
    "        \n",
    "        print(\"Computing the backward pass of module {}\".format(self.name))\n",
    "        \n",
    "        input = self.save_for_backward \n",
    "        grad_input = torch.matmul(grad_output, self.weight.data.t())\n",
    "        grad_weight = torch.matmul(input.t(), grad_output)\n",
    "        \n",
    "        print(\"input = {}, input.size = {}\".format(input, input.size()))\n",
    "        print(\"grad_output = {}, grad_output.size = {}\".format(grad_output, grad_output.size()))\n",
    "        print(\"grad_input = {}, grad_input.size = {}\".format(grad_input, grad_input.size()))\n",
    "        print(\"grad_weight = {}, grad_weight.size = {}\".format(grad_weight, grad_weight.size()))\n",
    "        \n",
    "        self.weight.grad += grad_weight\n",
    "        if self.bias: \n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "            self.bias.grad += grad_bias\n",
    "            \n",
    "        return grad_input \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        gain = calculate_gain('linear')\n",
    "        stdv = gain / math.sqrt(self.in_features)\n",
    "        bound = math.sqrt(3.0) * stdv\n",
    "        self.weight.data.uniform_(-bound, bound)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self, name):\n",
    "        super(ReLU, self).__init__(name)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        print(\"Applying module {} with input = \".format(self.name, input))\n",
    "        self.save_for_backward = input\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        input = self.save_for_backward\n",
    "        grad_input = grad_output.copy()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self, name=None):\n",
    "        if name is None: name = 'mse'\n",
    "        super(MSELoss, self).__init__(name)\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        \n",
    "        print(\"Computing forward pass of MSELoss\")\n",
    "        print(\"input = {}, input.size = {}\".format(input, input.size()))\n",
    "        print(\"target = {}, target.size = {}\".format(target, target.size()))\n",
    "        \n",
    "        assert(input.size() == target.size()), \"Input size different to target size.\"\n",
    "        self.save_for_backward_input = input\n",
    "        self.save_for_backward_target = target\n",
    "        se = (input - target)**2\n",
    "        \n",
    "        print(\"MSELoss = \", torch.mean(se))\n",
    "        return torch.mean(se)\n",
    "\n",
    "    def backward(self, grad_output=None):\n",
    "        \n",
    "        print(\"Computing backward pass of MSELoss\")\n",
    "        \n",
    "        input = self.save_for_backward_input\n",
    "        target = self.save_for_backward_target\n",
    "        grad_se = 2*(input - target) / len(input)\n",
    "        \n",
    "        print(\"Derivative wrt to input = \", grad_se)\n",
    "        return grad_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gain(nonlinearity='relu'):\n",
    "    linear_fns = ['linear', 'conv1d']\n",
    "    if nonlinearity in linear_fns or nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    else:\n",
    "        raise ValueEroor(\"Specified non-linearity is not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, model, defaults):\n",
    "        self.defaults = defaults\n",
    "        self.model = model\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p_group in self.model.param():\n",
    "            for key, p in p_group.items():\n",
    "                if p.grad is not None:\n",
    "                    p.set_grad_zero()\n",
    "                \n",
    "    def step(self, closure):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=0.01):\n",
    "        defaults = dict(lr=lr)\n",
    "        self.lr = lr\n",
    "        super(SGD, self).__init__(model, defaults)\n",
    "        \n",
    "    def step(self, closure):\n",
    "        loss= None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "    \n",
    "        for p_group in self.model.param():\n",
    "            for key, p in p_group.items():\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                p.data -= self.lr*d_p\n",
    "        \n",
    "        return loss       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f7806867eb8>), ('bias', <__main__.Parameter object at 0x7f7806867f98>)])\n",
      "param = <__main__.Parameter object at 0x7f7806867eb8>, param.data = tensor([[-0.0075],\n",
      "        [ 0.5364],\n",
      "        [-0.8230]])\n",
      "param = <__main__.Parameter object at 0x7f7806867f98>, param.data = tensor([-0.7359])\n",
      "\n",
      "MODULE.PARAM\n",
      "<generator object Module.param_per_module at 0x7f78068704f8>\n",
      "\n",
      "Computing the forward pass of module fc1 \n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "weight = <__main__.Parameter object at 0x7f7806867eb8>, weight.size = torch.Size([3, 1])\n",
      "bias = <__main__.Parameter object at 0x7f7806867f98>, bias.size = torch.Size([1])\n",
      "output = tensor([[-3.5434],\n",
      "        [-3.5359]]), output.size = <built-in method size of Tensor object at 0x7f780686d1b0>\n",
      "output =  tensor([[-3.5434],\n",
      "        [-3.5359]])\n",
      "\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867eb8> to zero\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867f98> to zero\n",
      "\n",
      "Computing the forward pass of module fc1 \n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "weight = <__main__.Parameter object at 0x7f7806867eb8>, weight.size = torch.Size([3, 1])\n",
      "bias = <__main__.Parameter object at 0x7f7806867f98>, bias.size = torch.Size([1])\n",
      "output = tensor([[-3.5434],\n",
      "        [-3.5359]]), output.size = <built-in method size of Tensor object at 0x7f780686d120>\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[-3.5434],\n",
      "        [-3.5359]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(25.6520)\n",
      "loss =  tensor(25.6520)\n",
      "Computing backward pass of MSELoss\n",
      "Derivative wrt to input =  tensor([[-5.5434],\n",
      "        [-4.5359]])\n",
      "Computing the backward pass of module fc1\n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "grad_output = tensor([[-5.5434],\n",
      "        [-4.5359]]), grad_output.size = torch.Size([2, 1])\n",
      "grad_input = tensor([[ 0.0415, -2.9737,  4.5625],\n",
      "        [ 0.0340, -2.4333,  3.7333]]), grad_input.size = torch.Size([2, 3])\n",
      "grad_weight = tensor([[-15.6227],\n",
      "        [-40.3173],\n",
      "        [-60.4760]]), grad_weight.size = torch.Size([3, 1])\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[-3.5434],\n",
      "        [-3.5359]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(25.6520)\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867eb8> to zero\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867f98> to zero\n",
      "\n",
      "Computing the forward pass of module fc1 \n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "weight = <__main__.Parameter object at 0x7f7806867eb8>, weight.size = torch.Size([3, 1])\n",
      "bias = <__main__.Parameter object at 0x7f7806867f98>, bias.size = torch.Size([1])\n",
      "output = tensor([[2.1111],\n",
      "        [1.9624]]), output.size = <built-in method size of Tensor object at 0x7f780686d900>\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[2.1111],\n",
      "        [1.9624]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.4692)\n",
      "loss =  tensor(0.4692)\n",
      "Computing backward pass of MSELoss\n",
      "Derivative wrt to input =  tensor([[0.1111],\n",
      "        [0.9624]])\n",
      "Computing the backward pass of module fc1\n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "grad_output = tensor([[0.1111],\n",
      "        [0.9624]]), grad_output.size = torch.Size([2, 1])\n",
      "grad_input = tensor([[ 0.0165,  0.1044, -0.0242],\n",
      "        [ 0.1431,  0.9042, -0.2101]]), grad_input.size = torch.Size([2, 3])\n",
      "grad_weight = tensor([[1.1845],\n",
      "        [4.2938],\n",
      "        [6.4406]]), grad_weight.size = torch.Size([3, 1])\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[2.1111],\n",
      "        [1.9624]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.4692)\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867eb8> to zero\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867f98> to zero\n",
      "\n",
      "Computing the forward pass of module fc1 \n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "weight = <__main__.Parameter object at 0x7f7806867eb8>, weight.size = torch.Size([3, 1])\n",
      "bias = <__main__.Parameter object at 0x7f7806867f98>, bias.size = torch.Size([1])\n",
      "output = tensor([[1.5185],\n",
      "        [1.3816]]), output.size = <built-in method size of Tensor object at 0x7f780686d9d8>\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5185],\n",
      "        [1.3816]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1887)\n",
      "loss =  tensor(0.1887)\n",
      "Computing backward pass of MSELoss\n",
      "Derivative wrt to input =  tensor([[-0.4815],\n",
      "        [ 0.3816]])\n",
      "Computing the backward pass of module fc1\n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "grad_output = tensor([[-0.4815],\n",
      "        [ 0.3816]]), grad_output.size = torch.Size([2, 1])\n",
      "grad_input = tensor([[-0.0659, -0.4318,  0.1361],\n",
      "        [ 0.0522,  0.3422, -0.1079]]), grad_input.size = torch.Size([2, 3])\n",
      "grad_weight = tensor([[-0.5815],\n",
      "        [-0.3998],\n",
      "        [-0.5997]]), grad_weight.size = torch.Size([3, 1])\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5185],\n",
      "        [1.3816]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1887)\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867eb8> to zero\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867f98> to zero\n",
      "\n",
      "Computing the forward pass of module fc1 \n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "weight = <__main__.Parameter object at 0x7f7806867eb8>, weight.size = torch.Size([3, 1])\n",
      "bias = <__main__.Parameter object at 0x7f7806867f98>, bias.size = torch.Size([1])\n",
      "output = tensor([[1.5831],\n",
      "        [1.4404]]), output.size = <built-in method size of Tensor object at 0x7f780686d8b8>\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5831],\n",
      "        [1.4404]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1839)\n",
      "loss =  tensor(0.1839)\n",
      "Computing backward pass of MSELoss\n",
      "Derivative wrt to input =  tensor([[-0.4169],\n",
      "        [ 0.4404]])\n",
      "Computing the backward pass of module fc1\n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "grad_output = tensor([[-0.4169],\n",
      "        [ 0.4404]]), grad_output.size = torch.Size([2, 1])\n",
      "grad_input = tensor([[-0.0595, -0.3755,  0.1154],\n",
      "        [ 0.0628,  0.3966, -0.1218]]), grad_input.size = torch.Size([2, 3])\n",
      "grad_weight = tensor([[-0.3935],\n",
      "        [ 0.0938],\n",
      "        [ 0.1406]]), grad_weight.size = torch.Size([3, 1])\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5831],\n",
      "        [1.4404]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1839)\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867eb8> to zero\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867f98> to zero\n",
      "\n",
      "Computing the forward pass of module fc1 \n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "weight = <__main__.Parameter object at 0x7f7806867eb8>, weight.size = torch.Size([3, 1])\n",
      "bias = <__main__.Parameter object at 0x7f7806867f98>, bias.size = torch.Size([1])\n",
      "output = tensor([[1.5785],\n",
      "        [1.4319]]), output.size = <built-in method size of Tensor object at 0x7f780686d948>\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5785],\n",
      "        [1.4319]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1821)\n",
      "loss =  tensor(0.1821)\n",
      "Computing backward pass of MSELoss\n",
      "Derivative wrt to input =  tensor([[-0.4215],\n",
      "        [ 0.4319]])\n",
      "Computing the backward pass of module fc1\n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "grad_output = tensor([[-0.4215],\n",
      "        [ 0.4319]]), grad_output.size = torch.Size([2, 1])\n",
      "grad_input = tensor([[-0.0618, -0.3792,  0.1172],\n",
      "        [ 0.0633,  0.3886, -0.1201]]), grad_input.size = torch.Size([2, 3])\n",
      "grad_weight = tensor([[-0.4111],\n",
      "        [ 0.0416],\n",
      "        [ 0.0624]]), grad_weight.size = torch.Size([3, 1])\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5785],\n",
      "        [1.4319]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1821)\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867eb8> to zero\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867f98> to zero\n",
      "\n",
      "Computing the forward pass of module fc1 \n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "weight = <__main__.Parameter object at 0x7f7806867eb8>, weight.size = torch.Size([3, 1])\n",
      "bias = <__main__.Parameter object at 0x7f7806867f98>, bias.size = torch.Size([1])\n",
      "output = tensor([[1.5812],\n",
      "        [1.4305]]), output.size = <built-in method size of Tensor object at 0x7f780686d1b0>\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5812],\n",
      "        [1.4305]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1803)\n",
      "loss =  tensor(0.1803)\n",
      "Computing backward pass of MSELoss\n",
      "Derivative wrt to input =  tensor([[-0.4188],\n",
      "        [ 0.4305]])\n",
      "Computing the backward pass of module fc1\n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "grad_output = tensor([[-0.4188],\n",
      "        [ 0.4305]]), grad_output.size = torch.Size([2, 1])\n",
      "grad_input = tensor([[-0.0631, -0.3766,  0.1167],\n",
      "        [ 0.0649,  0.3871, -0.1200]]), grad_input.size = torch.Size([2, 3])\n",
      "grad_weight = tensor([[-0.4071],\n",
      "        [ 0.0468],\n",
      "        [ 0.0702]]), grad_weight.size = torch.Size([3, 1])\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5812],\n",
      "        [1.4305]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1803)\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867eb8> to zero\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867f98> to zero\n",
      "\n",
      "Computing the forward pass of module fc1 \n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "weight = <__main__.Parameter object at 0x7f7806867eb8>, weight.size = torch.Size([3, 1])\n",
      "bias = <__main__.Parameter object at 0x7f7806867f98>, bias.size = torch.Size([1])\n",
      "output = tensor([[1.5832],\n",
      "        [1.4283]]), output.size = <built-in method size of Tensor object at 0x7f780686d990>\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5832],\n",
      "        [1.4283]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1786)\n",
      "loss =  tensor(0.1786)\n",
      "Computing backward pass of MSELoss\n",
      "Derivative wrt to input =  tensor([[-0.4168],\n",
      "        [ 0.4283]])\n",
      "Computing the backward pass of module fc1\n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "grad_output = tensor([[-0.4168],\n",
      "        [ 0.4283]]), grad_output.size = torch.Size([2, 1])\n",
      "grad_input = tensor([[-0.0645, -0.3747,  0.1165],\n",
      "        [ 0.0663,  0.3850, -0.1197]]), grad_input.size = torch.Size([2, 3])\n",
      "grad_weight = tensor([[-0.4053],\n",
      "        [ 0.0460],\n",
      "        [ 0.0691]]), grad_weight.size = torch.Size([3, 1])\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5832],\n",
      "        [1.4283]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1786)\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867eb8> to zero\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867f98> to zero\n",
      "\n",
      "Computing the forward pass of module fc1 \n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "weight = <__main__.Parameter object at 0x7f7806867eb8>, weight.size = torch.Size([3, 1])\n",
      "bias = <__main__.Parameter object at 0x7f7806867f98>, bias.size = torch.Size([1])\n",
      "output = tensor([[1.5852],\n",
      "        [1.4263]]), output.size = <built-in method size of Tensor object at 0x7f780686db40>\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5852],\n",
      "        [1.4263]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1769)\n",
      "loss =  tensor(0.1769)\n",
      "Computing backward pass of MSELoss\n",
      "Derivative wrt to input =  tensor([[-0.4148],\n",
      "        [ 0.4263]])\n",
      "Computing the backward pass of module fc1\n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "grad_output = tensor([[-0.4148],\n",
      "        [ 0.4263]]), grad_output.size = torch.Size([2, 1])\n",
      "grad_input = tensor([[-0.0659, -0.3727,  0.1162],\n",
      "        [ 0.0677,  0.3830, -0.1194]]), grad_input.size = torch.Size([2, 3])\n",
      "grad_weight = tensor([[-0.4034],\n",
      "        [ 0.0459],\n",
      "        [ 0.0688]]), grad_weight.size = torch.Size([3, 1])\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5852],\n",
      "        [1.4263]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1769)\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867eb8> to zero\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867f98> to zero\n",
      "\n",
      "Computing the forward pass of module fc1 \n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "weight = <__main__.Parameter object at 0x7f7806867eb8>, weight.size = torch.Size([3, 1])\n",
      "bias = <__main__.Parameter object at 0x7f7806867f98>, bias.size = torch.Size([1])\n",
      "output = tensor([[1.5872],\n",
      "        [1.4242]]), output.size = <built-in method size of Tensor object at 0x7f780686dab0>\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5872],\n",
      "        [1.4242]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1752)\n",
      "loss =  tensor(0.1752)\n",
      "Computing backward pass of MSELoss\n",
      "Derivative wrt to input =  tensor([[-0.4128],\n",
      "        [ 0.4242]])\n",
      "Computing the backward pass of module fc1\n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "grad_output = tensor([[-0.4128],\n",
      "        [ 0.4242]]), grad_output.size = torch.Size([2, 1])\n",
      "grad_input = tensor([[-0.0673, -0.3707,  0.1159],\n",
      "        [ 0.0691,  0.3809, -0.1191]]), grad_input.size = torch.Size([2, 3])\n",
      "grad_weight = tensor([[-0.4014],\n",
      "        [ 0.0457],\n",
      "        [ 0.0685]]), grad_weight.size = torch.Size([3, 1])\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5872],\n",
      "        [1.4242]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1752)\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867eb8> to zero\n",
      "setting grad of <__main__.Parameter object at 0x7f7806867f98> to zero\n",
      "\n",
      "Computing the forward pass of module fc1 \n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "weight = <__main__.Parameter object at 0x7f7806867eb8>, weight.size = torch.Size([3, 1])\n",
      "bias = <__main__.Parameter object at 0x7f7806867f98>, bias.size = torch.Size([1])\n",
      "output = tensor([[1.5891],\n",
      "        [1.4222]]), output.size = <built-in method size of Tensor object at 0x7f780686d480>\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5891],\n",
      "        [1.4222]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1735)\n",
      "loss =  tensor(0.1735)\n",
      "Computing backward pass of MSELoss\n",
      "Derivative wrt to input =  tensor([[-0.4109],\n",
      "        [ 0.4222]])\n",
      "Computing the backward pass of module fc1\n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "grad_output = tensor([[-0.4109],\n",
      "        [ 0.4222]]), grad_output.size = torch.Size([2, 1])\n",
      "grad_input = tensor([[-0.0686, -0.3687,  0.1157],\n",
      "        [ 0.0705,  0.3789, -0.1189]]), grad_input.size = torch.Size([2, 3])\n",
      "grad_weight = tensor([[-0.3995],\n",
      "        [ 0.0454],\n",
      "        [ 0.0681]]), grad_weight.size = torch.Size([3, 1])\n",
      "Computing forward pass of MSELoss\n",
      "input = tensor([[1.5891],\n",
      "        [1.4222]]), input.size = torch.Size([2, 1])\n",
      "target = tensor([[2.],\n",
      "        [1.]]), target.size = torch.Size([2, 1])\n",
      "MSELoss =  tensor(0.1735)\n",
      "\n",
      "\n",
      "Computing backward pass of MSELoss\n",
      "Derivative wrt to input =  tensor([[-0.4109],\n",
      "        [ 0.4222]])\n",
      "grad_output =  tensor([[-0.4109],\n",
      "        [ 0.4222]])\n",
      "Computing the backward pass of module fc1\n",
      "input = tensor([[2., 4., 6.],\n",
      "        [1., 4., 6.]]), input.size = torch.Size([2, 3])\n",
      "grad_output = tensor([[-0.4109],\n",
      "        [ 0.4222]]), grad_output.size = torch.Size([2, 1])\n",
      "grad_input = tensor([[-0.0702, -0.3685,  0.1159],\n",
      "        [ 0.0722,  0.3787, -0.1191]]), grad_input.size = torch.Size([2, 3])\n",
      "grad_weight = tensor([[-0.3995],\n",
      "        [ 0.0454],\n",
      "        [ 0.0681]]), grad_weight.size = torch.Size([3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0702, -0.3685,  0.1159],\n",
       "        [ 0.0722,  0.3787, -0.1191]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Linear('fc1', 3, 1)\n",
    "for p in model.param():\n",
    "    print(\"parameter p = \", p)\n",
    "    for key, param in p.items():\n",
    "        print(\"param = {}, param.data = {}\".format(param, param.data))\n",
    "\n",
    "print()\n",
    "print(\"MODULE.PARAM\")\n",
    "print(model.param())\n",
    "\n",
    "print()\n",
    "input = torch.Tensor([[2, 4, 6], [1, 4, 6]])\n",
    "output = model(input)\n",
    "print(\"output = \", output)\n",
    "target = torch.Tensor([[2], [1]])\n",
    "\n",
    "\n",
    "criterion = MSELoss()\n",
    "optimizer = SGD(model)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "print()\n",
    "nb_epochs = 10\n",
    "for e in range(nb_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    print()\n",
    "    \n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "    print(\"loss = \", loss)\n",
    "    grad_output = criterion.backward()\n",
    "    model.backward(grad_output)\n",
    "    optimizer.step(criterion(output, target))\n",
    "\n",
    "print()\n",
    "print()\n",
    "grad_output = criterion.backward()\n",
    "print(\"grad_output = \", grad_output)\n",
    "model.backward(grad_output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding child =  <__main__.Linear object at 0x7f780685db70>\n",
      "adding child =  <__main__.Linear object at 0x7f780685d4e0>\n",
      "adding child =  <__main__.ReLU object at 0x7f780685d160>\n",
      "MODULE.__call__\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5a647502a2b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MODULE.__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    Linear('fc1', 2, 4),\n",
    "    Linear('fc2', 4, 1),\n",
    "    ReLU('relu1')\n",
    "    )\n",
    "\n",
    "print(\"MODULE.__call__\")\n",
    "model(x)\n",
    "\n",
    "print()\n",
    "print(\"MODULE.PARAM\")\n",
    "for key, module in model._children.items():\n",
    "    print(\"module.param() = \", module.param())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second project, do we first accumulate the gradient then afterwards calculate the derivate of the loss wrt \n",
    "to the input.  Or do it the other way around.\n",
    "They are usually unrelated computations. Think about the following scenario. You have a batch of inputs x_0 to x_9. \n",
    "And a single parameter a. Thus the forward pass for this module is s_i = a*x_i. For the backward pass we get as \n",
    "input dl/ds_i for all i and we need to compute dl/da and dl/dx_i . It is quite obvious that \n",
    "dl/da = sum x_i * dl/ds_i for all i. And dl/dx_i = dl/ds_i * a. The order in which one computes the two is irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "x = torch.tensor([[1, 2], [2, 1], [3, 4]]).type(torch.FloatTensor).requires_grad_()\n",
    "y = torch.tensor([1, 0.4, 3])\n",
    "#x = torch.tensor([[1., 2.]]).requires_grad_()\n",
    "#y = torch.tensor([1.])\n",
    "\n",
    "model = nn.Sequential(nn.Linear(2, 10), nn.ReLU())\n",
    "\n",
    "print(\"PRINTING PARAMETERS\")\n",
    "for p in model.parameters():\n",
    "    print(\"p = \", p)\n",
    "y_pred = model(x)\n",
    "\n",
    "print(\"PRINTING PREDICTION\")\n",
    "print(\"y_pred = \", y_pred)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "loss = criterion(y_pred, y)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"PRINTING GRADIENT\")\n",
    "#print(\"loss.grad = \", autograd.grad(loss, x))\n",
    "loss.backward()\n",
    "for p in model.parameters():\n",
    "    print(\"p.grad = \", p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
