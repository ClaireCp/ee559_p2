{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fd3380a1470>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import Tensor\n",
    "torch.manual_seed(0)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\" Base class \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self._parameters = OrderedDict()\n",
    "        self._children = OrderedDict()\n",
    "        self.training = True\n",
    "        \n",
    "    def __call__(self, *input, **kwargs):\n",
    "        return self.forward(*input, **kwargs)\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *grad_output):\n",
    "        \"\"\" backward receives as input a pointer to a tensor or a tuple of tensors containing\n",
    "        the gradient of the loss (or the function of interest) wrt the module's output, accumulates\n",
    "        the gradient wrt the parameters, and returns a tensor or a tuple of tensors containing the \n",
    "        gradient of the loss wrt the module's input (Application of the chain rule)\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_children(self, module):\n",
    "        print(\"adding child = \", module)\n",
    "        assert isinstance(module, Module) and module is not None, \"Not a Module.\"\n",
    "        assert module.name not in self._children, \"Module {} already exists\".format(module.name)\n",
    "        self._children[module.name] = module\n",
    "        \n",
    "    def add_parameter(self, name, param):\n",
    "        assert isinstance(param, Parameter), \"Not a Parameter.\"\n",
    "        # check if parameter key is not already in the OrderedDict of parameters\n",
    "        assert name not in self._parameters, \"Parameter {} already exists\".format(name)\n",
    "        self._parameters[name] = param\n",
    "        \n",
    "    def param(self, recurse=True, verbose=False):\n",
    "        \"\"\" param returns a dict of Parameters, each composed of a parameter tensor, \n",
    "        and a gradient tensor of same size. This list is empty for parameterless modules. \"\"\"\n",
    "        \n",
    "        if recurse == False or isEmpty(self._children):\n",
    "            if verbose: print(\"Parameters of module \", self.name)\n",
    "            yield self._parameters\n",
    "        else:\n",
    "            \n",
    "            for key_mod, module in self._children.items():\n",
    "                yield module.param(recurse, verbose)\n",
    "                \n",
    "                    \n",
    "    def param_per_module(self):\n",
    "        if self._parameters:\n",
    "            yield self._parameters\n",
    "        else:\n",
    "            yield None\n",
    "            \n",
    "    def param_tree(self, verbose):\n",
    "        print(\"In param_tree of module {}._children = {}\".format(self.name, self._children))\n",
    "        for key_mod, module in self._children.items():\n",
    "                print(\"params of module = \", key_mod)\n",
    "                yield module.param_per_module(verbose)\n",
    "            \n",
    "def isEmpty(dict):\n",
    "    if dict: return False\n",
    "    else: return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__('seq_nn')\n",
    "        for index, module in enumerate(args):\n",
    "            print(\"Adding module = {} to children\".format(module.name))\n",
    "            self.add_children(module)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        self.save_for_backward = input\n",
    "        for key, module in self._children.items():\n",
    "            #print(\"Applying module = {}, with key = {}\".format(module.name, key))\n",
    "            input = module(input)\n",
    "        return input\n",
    "    \n",
    "    def backward(self, *grad_output):\n",
    "        for key, module in reversed(self._children.items()):\n",
    "            print(\"In Seq backward, grad_output = \", grad_output)\n",
    "            grad_output = module.backward(grad_output[0])\n",
    "            print(\"In Seq backward, grad_output = \", grad_output)\n",
    "        return grad_output      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Module may have tensor parameters, for each of which it should also have a \n",
    "# similar sized tensor gradient to accumulate the gradient during the backward pass\n",
    "class Parameter(object):\n",
    "    def __init__(self, tensor=None, grad=None, requires_grad=True):\n",
    "        assert tensor is None or isinstance(tensor, torch.Tensor), \"Not a tensor\"\n",
    "        self.data = tensor\n",
    "        self.grad = torch.empty(tensor.size())\n",
    "        self.requires_grad = requires_grad\n",
    "    \n",
    "    def set_data(self, tensor):\n",
    "        assert tensor is None or isinstance(tensor, torch.Tensor), \"Not a tensor\"\n",
    "        self.data = tensor  \n",
    "    \n",
    "    def set_grad_zero(self):\n",
    "        self.grad = torch.zeros(self.grad.size())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\" Implements a R^C -> R^D fully-connected layer:\n",
    "        Input: (N x C) tensor\n",
    "        Ouput: (N x D) tensor \"\"\"\n",
    "    def __init__(self, name, in_features, out_features, bias=True):\n",
    "        assert name is not None, \"Module that have parameters must have a unique name\"\n",
    "        super(Linear, self).__init__(name)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        self.reset_parameters()\n",
    "        self.add_parameter('weight', self.weight)\n",
    "        self.add_parameter('bias', self.bias)\n",
    "              \n",
    "    def forward(self, input):\n",
    "        self.save_for_backward = input\n",
    "        output = torch.matmul(input, self.weight.data)\n",
    "        if self.bias: \n",
    "            output += self.bias.data\n",
    "            \n",
    "        #print(\"Applying module {}, input = {}, output = {}\".format(self.name, input, output))\n",
    "        return output\n",
    "              \n",
    "    def backward(self, grad_output):\n",
    "        input = self.save_for_backward \n",
    "        print(\"In Linear backward, grad_ouput = \", grad_output)\n",
    "        print(\"self.weight.data.t() = \", self.weight.data.t())\n",
    "        print(grad_output.shape)\n",
    "        grad_input = torch.matmul(grad_output, self.weight.data.t())\n",
    "        grad_weight = torch.matmul(input.t(), grad_output)\n",
    "        self.weight.grad += grad_weight\n",
    "        if self.bias: \n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "            self.bias.grad += grad_bias          \n",
    "\n",
    "        #print(\"weight.grad = \", self.weight.grad.t())\n",
    "        return grad_input \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        gain = calculate_gain('linear')\n",
    "        stdv = gain / math.sqrt(self.in_features)\n",
    "        bound = math.sqrt(3.0) * stdv\n",
    "        self.weight.data.uniform_(-bound, bound)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self, name=None):\n",
    "        if name is None: name = 'relu'\n",
    "        super(ReLU, self).__init__(name)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.save_for_backward = input\n",
    "        #print(\"Applying ReLU, output = \", input.clamp(min=0))\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        input = self.save_for_backward\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "    \n",
    "class Tanh(Module):\n",
    "    def __init__(self, name=None):\n",
    "        if name is None: name = 'tanh'\n",
    "        super(Tanh, self).__init__(name)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.save_for_backward = input\n",
    "        return torch.tanh(input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        input = self.save_for_backward\n",
    "        grad_input = 1 - torch.tanh(input)**2\n",
    "        return grad_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self, name=None):\n",
    "        if name is None: name = 'mse'\n",
    "        super(MSELoss, self).__init__(name)\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        assert(input.size() == target.size()), \"Input size different to target size.\"\n",
    "        self.save_for_backward_input = input\n",
    "        self.save_for_backward_target = target\n",
    "        se = (input - target)**2\n",
    "        return torch.mean(se)\n",
    "\n",
    "    def backward(self, grad_output=None):\n",
    "        input = self.save_for_backward_input\n",
    "        target = self.save_for_backward_target\n",
    "        grad_se = 2*(input - target) / len(input)\n",
    "        return grad_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gain(nonlinearity='relu'):\n",
    "    linear_fns = ['linear', 'conv1d']\n",
    "    if nonlinearity in linear_fns or nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    else:\n",
    "        raise ValueEroor(\"Specified non-linearity is not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "class Optimizer(object):\n",
    "    def __init__(self, model, defaults):\n",
    "        self.defaults = defaults\n",
    "        self.model = model\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param_dict in self.model.param():\n",
    "            if isinstance(param_dict, types.GeneratorType): \n",
    "                print(\"IN ZERO_GRAD, param_dict = \", param_dict)\n",
    "                param_dict = next(param_dict)\n",
    "                print(param_dict)\n",
    "            for key, p in param_dict.items():\n",
    "                if p.grad is not None:\n",
    "                    p.set_grad_zero()\n",
    "                \n",
    "    def step(self, closure):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_parameters(model):\n",
    "    i = 0\n",
    "    for p in model_seq.param(verbose=True): # loop over generator object of each module\n",
    "        for param_dict in p: # get parameters OrderedDict()\n",
    "            if param_dict is not None:\n",
    "                for key, param in param_dict.items(): # loop over the parameters OrderedDict()\n",
    "                    if key == 'weight':\n",
    "                        if i == 0: init_weight1 = param.data.t().clone()\n",
    "                        if i == 1: init_weight2 = param.data.t().clone()    \n",
    "                    if key == 'bias':\n",
    "                        if i == 0: \n",
    "                            init_bias1 = param.data.clone()\n",
    "                            i += 1\n",
    "                        if i == 1: init_bias2 = param.data.clone()\n",
    "    return init_weight1, init_bias1, init_weight2, init_bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=0.01):\n",
    "        defaults = dict(lr=lr)\n",
    "        self.lr = lr\n",
    "        super(SGD, self).__init__(model, defaults)\n",
    "        \n",
    "    def step(self, closure):\n",
    "        loss= None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "    \n",
    "        for p_group in self.model.param():\n",
    "            for key, p in p_group.items():\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                p.data -= self.lr*d_p\n",
    "        \n",
    "        return loss       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of module  fc1\n",
      "p =  OrderedDict([('weight', <__main__.Parameter object at 0x7fd30b96bf28>), ('bias', <__main__.Parameter object at 0x7fd30b906da0>)])\n",
      "Parameter containing:\n",
      "tensor([[-0.0075,  0.5364, -0.8230]])\n",
      "Parameter containing:\n",
      "tensor([-0.7359])\n",
      "\n",
      "\n",
      "Adding module = fc1 to children\n",
      "adding child =  <__main__.Linear object at 0x7fd30b96b7f0>\n",
      "Adding module = fc2 to children\n",
      "adding child =  <__main__.Linear object at 0x7fd30b906f28>\n",
      "Adding module = relu1 to children\n",
      "adding child =  <__main__.ReLU object at 0x7fd30b906f60>\n",
      "\n",
      "\n",
      "<generator object Module.param at 0x7fd30b962930>\n",
      "Parameters of module  fc1\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7fd30b906cf8>), ('bias', <__main__.Parameter object at 0x7fd30b906e80>)])\n",
      "Parameter containing:\n",
      "tensor([[-0.3852, -0.0887, -0.9553],\n",
      "        [ 0.2682,  0.2646, -0.6623],\n",
      "        [-0.0198, -0.3022, -0.4122],\n",
      "        [ 0.7929, -0.1966,  0.0370]])\n",
      "Parameter containing:\n",
      "tensor([ 0.3953,  0.6000, -0.6779, -0.4355])\n",
      "<generator object Module.param at 0x7fd30b9629a8>\n",
      "Parameters of module  fc2\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7fd30b906e48>), ('bias', <__main__.Parameter object at 0x7fd30b906eb8>)])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3146,  0.7191, -0.1782,  0.6481]])\n",
      "Parameter containing:\n",
      "tensor([-0.1396])\n",
      "<generator object Module.param at 0x7fd30b962930>\n",
      "Parameters of module  relu1\n",
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "model = Linear('fc1', 3, 1)\n",
    "for p in model.param(verbose=True):\n",
    "    print(\"p = \", p)\n",
    "    for key, param in p.items():\n",
    "        print(\"Parameter containing:\")\n",
    "        if key == 'weight': print(param.data.t())\n",
    "        if key == 'bias': print(param.data)\n",
    " \n",
    "\n",
    "print()\n",
    "print()\n",
    "model_seq = Sequential(\n",
    "    Linear('fc1', 3, 4),\n",
    "    Linear('fc2', 4, 1),\n",
    "    ReLU('relu1')\n",
    "    )\n",
    "\n",
    "print()\n",
    "print()\n",
    "for p in model_seq.param(verbose=True):\n",
    "    print(p)\n",
    "    for param_dict in p:\n",
    "        print(param_dict)\n",
    "        if param_dict is not None:\n",
    "            for key, param in param_dict.items():\n",
    "                print(\"Parameter containing:\")\n",
    "                if key == 'weight': print(param.data.t())\n",
    "                if key == 'bias': print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', <__main__.Parameter object at 0x7fd30b96c1d0>), ('bias', <__main__.Parameter object at 0x7fd30b8f8860>)])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1058,  0.9055, -0.9277]])\n",
      "Parameter containing:\n",
      "tensor([-0.6295])\n",
      "\n",
      "e = 0, loss = 15.526599884033203\n",
      "grad_loss_wrt_output =  tensor([[-4.3620, -3.4678]])\n",
      "In Linear backward, grad_ouput =  tensor([[-4.3620],\n",
      "        [-3.4678]])\n",
      "self.weight.data.t() =  tensor([[ 0.1058,  0.9055, -0.9277]])\n",
      "torch.Size([2, 1])\n",
      "\n",
      "e = 1, loss = 0.3236342966556549\n",
      "grad_loss_wrt_output =  tensor([[0.0316, 0.8039]])\n",
      "In Linear backward, grad_ouput =  tensor([[0.0316],\n",
      "        [0.8039]])\n",
      "self.weight.data.t() =  tensor([[ 0.2277,  1.2187, -0.4579]])\n",
      "torch.Size([2, 1])\n",
      "\n",
      "e = 2, loss = 0.1539161503314972\n",
      "grad_loss_wrt_output =  tensor([[-0.4285,  0.3524]])\n",
      "In Linear backward, grad_ouput =  tensor([[-0.4285],\n",
      "        [ 0.3524]])\n",
      "self.weight.data.t() =  tensor([[ 0.2191,  1.1852, -0.5080]])\n",
      "torch.Size([2, 1])\n",
      "\n",
      "e = 3, loss = 0.15059930086135864\n",
      "grad_loss_wrt_output =  tensor([[-0.3781,  0.3978]])\n",
      "In Linear backward, grad_ouput =  tensor([[-0.3781],\n",
      "        [ 0.3978]])\n",
      "self.weight.data.t() =  tensor([[ 0.2241,  1.1883, -0.5034]])\n",
      "torch.Size([2, 1])\n",
      "\n",
      "e = 4, loss = 0.149137943983078\n",
      "grad_loss_wrt_output =  tensor([[-0.3814,  0.3909]])\n",
      "In Linear backward, grad_ouput =  tensor([[-0.3814],\n",
      "        [ 0.3909]])\n",
      "self.weight.data.t() =  tensor([[ 0.2277,  1.1875, -0.5046]])\n",
      "torch.Size([2, 1])\n",
      "\n",
      "\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.1058,  0.9055, -0.9277]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6295], requires_grad=True)\n",
      "\n",
      "e = 0, loss = 15.526599884033203\n",
      "grad_loss_wrt_output =  tensor([[-4.3620, -3.4678]])\n",
      "weight.grad =  tensor([[-12.1919, -31.3195, -46.9792]])\n",
      "\n",
      "e = 1, loss = 0.3236342966556549\n",
      "grad_loss_wrt_output =  tensor([[0.0316, 0.8039]])\n",
      "weight.grad =  tensor([[0.8672, 3.3422, 5.0133]])\n",
      "\n",
      "e = 2, loss = 0.1539161503314972\n",
      "grad_loss_wrt_output =  tensor([[-0.4285,  0.3524]])\n",
      "weight.grad =  tensor([[-0.5047, -0.3046, -0.4569]])\n",
      "\n",
      "e = 3, loss = 0.15059931576251984\n",
      "grad_loss_wrt_output =  tensor([[-0.3781,  0.3978]])\n",
      "weight.grad =  tensor([[-0.3584,  0.0788,  0.1183]])\n",
      "\n",
      "e = 4, loss = 0.149137943983078\n",
      "grad_loss_wrt_output =  tensor([[-0.3814,  0.3909]])\n",
      "weight.grad =  tensor([[-0.3718,  0.0383,  0.0574]])\n"
     ]
    }
   ],
   "source": [
    "model = Linear('fc1', 3, 1)\n",
    "init_weight = torch.zeros([1, 3])\n",
    "init_bias = torch.zeros([])\n",
    "for p in model.param():\n",
    "    print(p)\n",
    "    for key, param in p.items():\n",
    "        print(\"Parameter containing:\")\n",
    "        if key == 'weight': \n",
    "            print(param.data.t())\n",
    "            init_weight = param.data.t().clone()\n",
    "        if key == 'bias': \n",
    "            print(param.data)\n",
    "            init_bias = param.data.clone()\n",
    "\n",
    "input = torch.Tensor([[2., 4., 6.], [1., 4., 6.]])\n",
    "output = model(input)\n",
    "target = torch.Tensor([[2], [1]])\n",
    "\n",
    "criterion = MSELoss()\n",
    "optimizer = SGD(model)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "nb_epochs = 5\n",
    "for e in range(nb_epochs):\n",
    "    print()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "    print(\"e = {}, loss = {}\".format(e, loss))\n",
    "    grad_loss_wrt_output = criterion.backward()\n",
    "    print(\"grad_loss_wrt_output = \", grad_loss_wrt_output.t())\n",
    "    model.backward(grad_loss_wrt_output)\n",
    "    optimizer.step(criterion(output, target))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "torch.set_grad_enabled(True)\n",
    "input = torch.Tensor([[2., 4., 6.], [1., 4., 6.]])\n",
    "target = torch.Tensor([[2], [1]])\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(3, 1)\n",
    "        self.fc.weight.data = init_weight\n",
    "        self.fc.bias.data = init_bias\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# I want to print the gradient of the loss wrt to the model output. For this I register a backward hook.\n",
    "def hook(module, gradInput, gradOutput):\n",
    "    for grad in gradOutput:\n",
    "        print(\"grad_loss_wrt_output = \", grad.t())\n",
    "\n",
    "model_torch = Net()\n",
    "for p in model_torch.parameters():\n",
    "    print(p)\n",
    "model_torch.register_backward_hook(hook)     \n",
    "criterion_torch = nn.MSELoss()\n",
    "optimizer_torch = torch.optim.SGD(model_torch.parameters(), lr=0.01)\n",
    "for e in range(nb_epochs):\n",
    "    print()\n",
    "    optimizer_torch.zero_grad()\n",
    "    output = model_torch(input).requires_grad_()\n",
    "    loss_torch = criterion_torch(output, target)\n",
    "    print(\"e = {}, loss = {}\".format(e, loss_torch))\n",
    "    loss_torch.backward()\n",
    "    for p in model_torch.parameters():\n",
    "        print(\"weight.grad = \", p.grad)\n",
    "        break\n",
    "    optimizer_torch.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding module = fc1 to children\n",
      "adding child =  <__main__.Linear object at 0x7fd30b8f8160>\n",
      "Adding module = fc2 to children\n",
      "adding child =  <__main__.Linear object at 0x7fd30b9208d0>\n",
      "Adding module = relu1 to children\n",
      "adding child =  <__main__.ReLU object at 0x7fd30b920978>\n",
      "Parameters of module  fc1\n",
      "Parameters of module  fc2\n",
      "Parameters of module  relu1\n",
      "tensor([[ 0.3424,  0.5196, -0.5871, -0.3771]])\n",
      "\n",
      "IN ZERO_GRAD, param_dict =  <generator object Module.param at 0x7fd30b962e58>\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7fd30b920898>), ('bias', <__main__.Parameter object at 0x7fd30b9207f0>)])\n",
      "IN ZERO_GRAD, param_dict =  <generator object Module.param at 0x7fd30b962e58>\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7fd30b920908>), ('bias', <__main__.Parameter object at 0x7fd30b920940>)])\n",
      "IN ZERO_GRAD, param_dict =  <generator object Module.param at 0x7fd30b962e58>\n",
      "OrderedDict()\n",
      "e = 0, output = tensor([[2.9778],\n",
      "        [1.9408]])\n",
      "e = 0, loss = 0.920551061630249\n",
      "grad_loss_wrt_output =  tensor([[0.9778, 0.9408]])\n",
      "In Seq backward, grad_output =  (tensor([[0.9778],\n",
      "        [0.9408]]),)\n",
      "In Seq backward, grad_output =  tensor([[0.9778],\n",
      "        [0.9408]])\n",
      "In Seq backward, grad_output =  tensor([[0.9778],\n",
      "        [0.9408]])\n",
      "In Linear backward, grad_ouput =  tensor([0.9778])\n",
      "self.weight.data.t() =  tensor([[ 0.3424,  0.5196, -0.5871, -0.3771]])\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, [4 x 2], [1] at /opt/conda/conda-bld/pytorch-cpu_1549632688322/work/aten/src/TH/generic/THTensorMath.cpp:821",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-57a9c0cbcce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad_loss_wrt_output = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss_wrt_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmodel_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_loss_wrt_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ba19c49b8508>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, *grad_output)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In Seq backward, grad_output = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mgrad_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In Seq backward, grad_output = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-860c960120a6>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mgrad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mgrad_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrad_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, [4 x 2], [1] at /opt/conda/conda-bld/pytorch-cpu_1549632688322/work/aten/src/TH/generic/THTensorMath.cpp:821"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "torch.manual_seed(0)\n",
    "torch.set_grad_enabled(False)\n",
    "del nn\n",
    "del optim\n",
    "model_seq = Sequential(\n",
    "    Linear('fc1', 3, 4),\n",
    "    Linear('fc2', 4, 1),\n",
    "    ReLU('relu1')\n",
    "    )\n",
    "\n",
    "input = torch.Tensor([[2., 4., 6.], [1., 4., 6.]])\n",
    "target = torch.Tensor([[2], [1]])\n",
    "model_seq(input)\n",
    "\n",
    "def get_initial_parameters(model):\n",
    "    i = 0\n",
    "    for p in model_seq.param(verbose=True): # loop over generator object of each module\n",
    "        for param_dict in p: # get parameters OrderedDict()\n",
    "            if param_dict is not None:\n",
    "                for key, param in param_dict.items(): # loop over the parameters OrderedDict()\n",
    "                    if key == 'weight':\n",
    "                        if i == 0: init_weight1 = param.data.t().clone()\n",
    "                        if i == 1: init_weight2 = param.data.t().clone()    \n",
    "                    if key == 'bias':\n",
    "                        if i == 0: \n",
    "                            init_bias1 = param.data.clone()\n",
    "                            i += 1\n",
    "                        if i == 1: init_bias2 = param.data.clone()\n",
    "    return init_weight1, init_bias1, init_weight2, init_bias2\n",
    "init_weight1, init_bias1, init_weight2, init_bias2 = get_initial_parameters(model_seq)\n",
    "print(init_weight2)                       \n",
    "\n",
    "                    \n",
    "criterion = MSELoss()\n",
    "optimizer = SGD(model_seq)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "nb_epochs = 2\n",
    "for e in range(nb_epochs):\n",
    "    print()\n",
    "    optimizer.zero_grad()\n",
    "    output = model_seq(input)\n",
    "    print(\"e = {}, output = {}\".format(e, output))\n",
    "    loss = criterion(output, target)\n",
    "    print(\"e = {}, loss = {}\".format(e, loss))\n",
    "    \n",
    "    grad_loss_wrt_output = criterion.backward()\n",
    "    print(\"grad_loss_wrt_output = \", grad_loss_wrt_output.t())\n",
    "    \n",
    "    model_seq.backward(grad_loss_wrt_output)\n",
    "    optimizer.step(criterion(output, target))\n",
    "    \n",
    "print()\n",
    "print()\n",
    "print()\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "torch.set_grad_enabled(True)\n",
    "input = torch.Tensor([[2., 4., 6.], [1., 4., 6.]])\n",
    "target = torch.Tensor([[2], [1]])\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 4)\n",
    "        \n",
    "        print(self.fc1.weight.data.shape)\n",
    "        self.fc1.weight.data = init_weight1\n",
    "        \n",
    "        self.fc2 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc1.bias.data = init_bias1\n",
    "        self.fc2.weight.data = init_weight2\n",
    "        self.fc2.bias.data = init_bias2\n",
    "    def forward(self, x):\n",
    "        #print(\"x0 = \", x)\n",
    "        x = self.fc1(x)\n",
    "        #print(\"x1 = \", x)\n",
    "        x = self.fc2(x)\n",
    "        #print(\"x2 = \", x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"x3 = \", x)\n",
    "        return x\n",
    "    \n",
    "# I want to print the gradient of the loss wrt to the model output. For this I register a backward hook.\n",
    "def hook(module, gradInput, gradOutput):\n",
    "    for grad in gradOutput:\n",
    "        print(\"grad_loss_wrt_output = \", grad.t())\n",
    "        \n",
    "model_torch = Net()\n",
    "for p in model_torch.parameters():\n",
    "    print(p)\n",
    "model_torch.register_backward_hook(hook)     \n",
    "criterion_torch = nn.MSELoss()\n",
    "optimizer_torch = torch.optim.SGD(model_torch.parameters(), lr=0.01)\n",
    "for e in range(nb_epochs):\n",
    "    print()\n",
    "    optimizer_torch.zero_grad()\n",
    "    output = model_torch(input).requires_grad_()\n",
    "    print(\"e = {}, output = {}\".format(e, output))\n",
    "    loss_torch = criterion_torch(output, target)\n",
    "    print(\"e = {}, loss = {}\".format(e, loss_torch))\n",
    "    loss_torch.backward()\n",
    "    #for p in model_torch.parameters():\n",
    "    #    print(p)\n",
    "    #    print(\"weight.grad = \", p.grad)\n",
    "    #    break\n",
    "    optimizer_torch.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "torch.set_grad_enabled(True)\n",
    "model_seq_torch = nn.Sequential(nn.Linear(3, 4), \n",
    "                            nn.Linear(4, 1), \n",
    "                            nn.ReLU())\n",
    "model_seq_torch(input)\n",
    "for p in model_seq_torch.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"MODULE.PARAM\")\n",
    "for key, module in model_seq._children.items():\n",
    "    print(\"module.param() = \", module.param())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second project, do we first accumulate the gradient then afterwards calculate the derivate of the loss wrt \n",
    "to the input.  Or do it the other way around.\n",
    "They are usually unrelated computations. Think about the following scenario. You have a batch of inputs x_0 to x_9. \n",
    "And a single parameter a. Thus the forward pass for this module is s_i = a*x_i. For the backward pass we get as \n",
    "input dl/ds_i for all i and we need to compute dl/da and dl/dx_i . It is quite obvious that \n",
    "dl/da = sum x_i * dl/ds_i for all i. And dl/dx_i = dl/ds_i * a. The order in which one computes the two is irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "x = torch.tensor([[1, 2], [2, 1], [3, 4]]).type(torch.FloatTensor).requires_grad_()\n",
    "y = torch.tensor([1, 0.4, 3])\n",
    "#x = torch.tensor([[1., 2.]]).requires_grad_()\n",
    "#y = torch.tensor([1.])\n",
    "\n",
    "model = nn.Sequential(nn.Linear(2, 10), nn.ReLU())\n",
    "\n",
    "print(\"PRINTING PARAMETERS\")\n",
    "for p in model.parameters():\n",
    "    print(\"p = \", p)\n",
    "y_pred = model(x)\n",
    "\n",
    "print(\"PRINTING PREDICTION\")\n",
    "print(\"y_pred = \", y_pred)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "loss = criterion(y_pred, y)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"PRINTING GRADIENT\")\n",
    "#print(\"loss.grad = \", autograd.grad(loss, x))\n",
    "loss.backward()\n",
    "for p in model.parameters():\n",
    "    print(\"p.grad = \", p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
