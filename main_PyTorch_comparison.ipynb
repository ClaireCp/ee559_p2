{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f86c77084e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import Tensor\n",
    "torch.manual_seed(0)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\" Base class \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self._parameters = OrderedDict()\n",
    "        self._children = OrderedDict()\n",
    "        self.training = True\n",
    "        \n",
    "    def __call__(self, *input, **kwargs):\n",
    "        return self.forward(*input, **kwargs)\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *grad_output):\n",
    "        \"\"\" backward receives as input a pointer to a tensor or a tuple of tensors containing\n",
    "        the gradient of the loss (or the function of interest) wrt the module's output, accumulates\n",
    "        the gradient wrt the parameters, and returns a tensor or a tuple of tensors containing the \n",
    "        gradient of the loss wrt the module's input (Application of the chain rule)\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def add_children(self, module):\n",
    "        print(\"adding child = \", module)\n",
    "        assert isinstance(module, Module) and module is not None, \"Not a Module.\"\n",
    "        assert module.name not in self._children, \"Module {} already exists\".format(module.name)\n",
    "        self._children[module.name] = module\n",
    "        \n",
    "    def add_parameter(self, name, param):\n",
    "        assert isinstance(param, Parameter), \"Not a Parameter.\"\n",
    "        # check if parameter key is not already in the OrderedDict of parameters\n",
    "        assert name not in self._parameters, \"Parameter {} already exists\".format(name)\n",
    "        self._parameters[name] = param\n",
    "        \n",
    "    def param(self, recurse=True, verbose=False):\n",
    "        \"\"\" param returns a dict of Parameters, each composed of a parameter tensor, \n",
    "        and a gradient tensor of same size. This list is empty for parameterless modules. \"\"\"\n",
    "        \n",
    "        if recurse == False or isEmpty(self._children):\n",
    "            if verbose: print(\"Parameters of module \", self.name)\n",
    "            yield self._parameters\n",
    "        else:\n",
    "            \n",
    "            for key_mod, module in self._children.items():\n",
    "                yield module.param(recurse, verbose)\n",
    "                \n",
    "                    \n",
    "    def param_per_module(self):\n",
    "        if self._parameters:\n",
    "            yield self._parameters\n",
    "        else:\n",
    "            yield None\n",
    "            \n",
    "    def param_tree(self, verbose):\n",
    "        print(\"In param_tree of module {}._children = {}\".format(self.name, self._children))\n",
    "        for key_mod, module in self._children.items():\n",
    "                print(\"params of module = \", key_mod)\n",
    "                yield module.param_per_module(verbose)\n",
    "            \n",
    "def isEmpty(dict):\n",
    "    if dict: return False\n",
    "    else: return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__('seq_nn')\n",
    "        for index, module in enumerate(args):\n",
    "            print(\"Adding module = {} to children\".format(module.name))\n",
    "            self.add_children(module)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        self.save_for_backward = input\n",
    "        for key, module in self._children.items():\n",
    "            #print(\"Applying module = {}, with key = {}\".format(module.name, key))\n",
    "            input = module(input)\n",
    "        return input\n",
    "    \n",
    "    def backward(self, *grad_output):\n",
    "        for key, module in reversed(self._children.items()):\n",
    "            print(\"In Seq backward, grad_output = \", grad_output)\n",
    "            if isinstance(grad_output, tuple): grad_output = grad_output[0]\n",
    "            grad_output = module.backward(grad_output)\n",
    "            print(\"In Seq backward, grad_output = \", grad_output)\n",
    "        return grad_output      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Module may have tensor parameters, for each of which it should also have a \n",
    "# similar sized tensor gradient to accumulate the gradient during the backward pass\n",
    "class Parameter(object):\n",
    "    def __init__(self, tensor=None, grad=None, requires_grad=True):\n",
    "        assert tensor is None or isinstance(tensor, torch.Tensor), \"Not a tensor\"\n",
    "        self.data = tensor\n",
    "        self.grad = torch.empty(tensor.size())\n",
    "        self.requires_grad = requires_grad\n",
    "    \n",
    "    def set_data(self, tensor):\n",
    "        assert tensor is None or isinstance(tensor, torch.Tensor), \"Not a tensor\"\n",
    "        self.data = tensor  \n",
    "    \n",
    "    def set_grad_zero(self):\n",
    "        self.grad = torch.zeros(self.grad.size())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\" Implements a R^C -> R^D fully-connected layer:\n",
    "        Input: (N x C) tensor\n",
    "        Ouput: (N x D) tensor \"\"\"\n",
    "    def __init__(self, name, in_features, out_features, bias=True):\n",
    "        assert name is not None, \"Module that have parameters must have a unique name\"\n",
    "        super(Linear, self).__init__(name)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        self.reset_parameters()\n",
    "        self.add_parameter('weight', self.weight)\n",
    "        self.add_parameter('bias', self.bias)\n",
    "              \n",
    "    def forward(self, input):\n",
    "        self.save_for_backward = input\n",
    "        output = torch.matmul(input, self.weight.data)\n",
    "        if self.bias: \n",
    "            output += self.bias.data\n",
    "            \n",
    "        #print(\"Applying module {}, input = {}, output = {}\".format(self.name, input, output))\n",
    "        return output\n",
    "              \n",
    "    def backward(self, grad_output):\n",
    "        input = self.save_for_backward \n",
    "        print(\"In Linear backward, grad_ouput = \", grad_output)\n",
    "        print(\"self.weight.data.t() = \", self.weight.data.t())\n",
    "        print(grad_output.shape)\n",
    "        grad_input = torch.matmul(grad_output, self.weight.data.t())\n",
    "        grad_weight = torch.matmul(input.t(), grad_output)\n",
    "        self.weight.grad += grad_weight\n",
    "        if self.bias: \n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "            self.bias.grad += grad_bias          \n",
    "\n",
    "        #print(\"weight.grad = \", self.weight.grad.t())\n",
    "        return grad_input \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        gain = calculate_gain('linear')\n",
    "        stdv = gain / math.sqrt(self.in_features)\n",
    "        bound = math.sqrt(3.0) * stdv\n",
    "        self.weight.data.uniform_(-bound, bound)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self, name=None):\n",
    "        if name is None: name = 'relu'\n",
    "        super(ReLU, self).__init__(name)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.save_for_backward = input\n",
    "        #print(\"Applying ReLU, output = \", input.clamp(min=0))\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        input = self.save_for_backward\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "    \n",
    "class Tanh(Module):\n",
    "    def __init__(self, name=None):\n",
    "        if name is None: name = 'tanh'\n",
    "        super(Tanh, self).__init__(name)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.save_for_backward = input\n",
    "        return torch.tanh(input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        input = self.save_for_backward\n",
    "        grad_input = 1 - torch.tanh(input)**2\n",
    "        return grad_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self, name=None):\n",
    "        if name is None: name = 'mse'\n",
    "        super(MSELoss, self).__init__(name)\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        assert(input.size() == target.size()), \"Input size different to target size.\"\n",
    "        self.save_for_backward_input = input\n",
    "        self.save_for_backward_target = target\n",
    "        se = (input - target)**2\n",
    "        return torch.mean(se)\n",
    "\n",
    "    def backward(self, grad_output=None):\n",
    "        input = self.save_for_backward_input\n",
    "        target = self.save_for_backward_target\n",
    "        grad_se = 2*(input - target) / len(input)\n",
    "        return grad_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gain(nonlinearity='relu'):\n",
    "    linear_fns = ['linear', 'conv1d']\n",
    "    if nonlinearity in linear_fns or nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    else:\n",
    "        raise ValueEroor(\"Specified non-linearity is not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "class Optimizer(object):\n",
    "    def __init__(self, model, defaults):\n",
    "        self.defaults = defaults\n",
    "        self.model = model\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param_dict in self.model.param():\n",
    "            if isinstance(param_dict, types.GeneratorType): \n",
    "                print(\"IN ZERO_GRAD, param_dict = \", param_dict)\n",
    "                param_dict = next(param_dict)\n",
    "                print(param_dict)\n",
    "            for key, p in param_dict.items():\n",
    "                if p.grad is not None:\n",
    "                    p.set_grad_zero()\n",
    "                \n",
    "    def step(self, closure):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, model, lr=0.01):\n",
    "        defaults = dict(lr=lr)\n",
    "        self.lr = lr\n",
    "        super(SGD, self).__init__(model, defaults)\n",
    "        \n",
    "    def step(self, closure):\n",
    "        loss= None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "    \n",
    "        for param_dict in self.model.param():\n",
    "            if isinstance(param_dict, types.GeneratorType):\n",
    "                print(\"IN STEP, param_dict = \", param_dict)\n",
    "                param_dict = next(param_dict)\n",
    "                print(param_dict)\n",
    "            for key, p in param_dict.items():\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                p.data -= self.lr*d_p\n",
    "        \n",
    "        return loss       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_parameters(model):\n",
    "    i = 0\n",
    "    for p in model_seq.param(verbose=True): # loop over generator object of each module\n",
    "        for param_dict in p: # get parameters OrderedDict()\n",
    "            if param_dict is not None:\n",
    "                for key, param in param_dict.items(): # loop over the parameters OrderedDict()\n",
    "                    if key == 'weight':\n",
    "                        if i == 0: init_weight1 = param.data.t().clone()\n",
    "                        if i == 1: init_weight2 = param.data.t().clone()    \n",
    "                    if key == 'bias':\n",
    "                        if i == 0: \n",
    "                            init_bias1 = param.data.clone()\n",
    "                            i += 1\n",
    "                        if i == 1: init_bias2 = param.data.clone()\n",
    "    return init_weight1, init_bias1, init_weight2, init_bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of module  fc1\n",
      "p =  OrderedDict([('weight', <__main__.Parameter object at 0x7f86b07c1cc0>), ('bias', <__main__.Parameter object at 0x7f86b07c1cf8>)])\n",
      "Parameter containing:\n",
      "tensor([[-0.0075,  0.5364, -0.8230]])\n",
      "Parameter containing:\n",
      "tensor([-0.7359])\n",
      "\n",
      "\n",
      "Adding module = fc1 to children\n",
      "adding child =  <__main__.Linear object at 0x7f86b081ae48>\n",
      "Adding module = fc2 to children\n",
      "adding child =  <__main__.Linear object at 0x7f86b07c1c50>\n",
      "Adding module = relu1 to children\n",
      "adding child =  <__main__.ReLU object at 0x7f86b07c1c18>\n",
      "\n",
      "\n",
      "<generator object Module.param at 0x7f86b08109a8>\n",
      "Parameters of module  fc1\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7f86b081a710>), ('bias', <__main__.Parameter object at 0x7f86b07c1ba8>)])\n",
      "Parameter containing:\n",
      "tensor([[-0.3852, -0.0887, -0.9553],\n",
      "        [ 0.2682,  0.2646, -0.6623],\n",
      "        [-0.0198, -0.3022, -0.4122],\n",
      "        [ 0.7929, -0.1966,  0.0370]])\n",
      "Parameter containing:\n",
      "tensor([ 0.3953,  0.6000, -0.6779, -0.4355])\n",
      "<generator object Module.param at 0x7f86b0810a20>\n",
      "Parameters of module  fc2\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7f86b07c1c88>), ('bias', <__main__.Parameter object at 0x7f86b07c1be0>)])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3146,  0.7191, -0.1782,  0.6481]])\n",
      "Parameter containing:\n",
      "tensor([-0.1396])\n",
      "<generator object Module.param at 0x7f86b08109a8>\n",
      "Parameters of module  relu1\n",
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "model = Linear('fc1', 3, 1)\n",
    "for p in model.param(verbose=True):\n",
    "    print(\"p = \", p)\n",
    "    for key, param in p.items():\n",
    "        print(\"Parameter containing:\")\n",
    "        if key == 'weight': print(param.data.t())\n",
    "        if key == 'bias': print(param.data)\n",
    " \n",
    "\n",
    "print()\n",
    "print()\n",
    "model_seq = Sequential(\n",
    "    Linear('fc1', 3, 4),\n",
    "    Linear('fc2', 4, 1),\n",
    "    ReLU('relu1')\n",
    "    )\n",
    "\n",
    "print()\n",
    "print()\n",
    "for p in model_seq.param(verbose=True):\n",
    "    print(p)\n",
    "    for param_dict in p:\n",
    "        print(param_dict)\n",
    "        if param_dict is not None:\n",
    "            for key, param in param_dict.items():\n",
    "                print(\"Parameter containing:\")\n",
    "                if key == 'weight': print(param.data.t())\n",
    "                if key == 'bias': print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', <__main__.Parameter object at 0x7f86b07af710>), ('bias', <__main__.Parameter object at 0x7f86b081a7b8>)])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1058,  0.9055, -0.9277]])\n",
      "Parameter containing:\n",
      "tensor([-0.6295])\n",
      "\n",
      "e = 0, loss = 15.526599884033203\n",
      "grad_loss_wrt_output =  tensor([[-4.3620, -3.4678]])\n",
      "In Linear backward, grad_ouput =  tensor([[-4.3620],\n",
      "        [-3.4678]])\n",
      "self.weight.data.t() =  tensor([[ 0.1058,  0.9055, -0.9277]])\n",
      "torch.Size([2, 1])\n",
      "\n",
      "e = 1, loss = 0.3236342966556549\n",
      "grad_loss_wrt_output =  tensor([[0.0316, 0.8039]])\n",
      "In Linear backward, grad_ouput =  tensor([[0.0316],\n",
      "        [0.8039]])\n",
      "self.weight.data.t() =  tensor([[ 0.2277,  1.2187, -0.4579]])\n",
      "torch.Size([2, 1])\n",
      "\n",
      "e = 2, loss = 0.1539161503314972\n",
      "grad_loss_wrt_output =  tensor([[-0.4285,  0.3524]])\n",
      "In Linear backward, grad_ouput =  tensor([[-0.4285],\n",
      "        [ 0.3524]])\n",
      "self.weight.data.t() =  tensor([[ 0.2191,  1.1852, -0.5080]])\n",
      "torch.Size([2, 1])\n",
      "\n",
      "e = 3, loss = 0.15059930086135864\n",
      "grad_loss_wrt_output =  tensor([[-0.3781,  0.3978]])\n",
      "In Linear backward, grad_ouput =  tensor([[-0.3781],\n",
      "        [ 0.3978]])\n",
      "self.weight.data.t() =  tensor([[ 0.2241,  1.1883, -0.5034]])\n",
      "torch.Size([2, 1])\n",
      "\n",
      "e = 4, loss = 0.149137943983078\n",
      "grad_loss_wrt_output =  tensor([[-0.3814,  0.3909]])\n",
      "In Linear backward, grad_ouput =  tensor([[-0.3814],\n",
      "        [ 0.3909]])\n",
      "self.weight.data.t() =  tensor([[ 0.2277,  1.1875, -0.5046]])\n",
      "torch.Size([2, 1])\n",
      "\n",
      "\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.1058,  0.9055, -0.9277]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6295], requires_grad=True)\n",
      "\n",
      "e = 0, loss = 15.526599884033203\n",
      "grad_loss_wrt_output =  tensor([[-4.3620, -3.4678]])\n",
      "weight.grad =  tensor([[-12.1919, -31.3195, -46.9792]])\n",
      "\n",
      "e = 1, loss = 0.3236342966556549\n",
      "grad_loss_wrt_output =  tensor([[0.0316, 0.8039]])\n",
      "weight.grad =  tensor([[0.8672, 3.3422, 5.0133]])\n",
      "\n",
      "e = 2, loss = 0.1539161503314972\n",
      "grad_loss_wrt_output =  tensor([[-0.4285,  0.3524]])\n",
      "weight.grad =  tensor([[-0.5047, -0.3046, -0.4569]])\n",
      "\n",
      "e = 3, loss = 0.15059931576251984\n",
      "grad_loss_wrt_output =  tensor([[-0.3781,  0.3978]])\n",
      "weight.grad =  tensor([[-0.3584,  0.0788,  0.1183]])\n",
      "\n",
      "e = 4, loss = 0.149137943983078\n",
      "grad_loss_wrt_output =  tensor([[-0.3814,  0.3909]])\n",
      "weight.grad =  tensor([[-0.3718,  0.0383,  0.0574]])\n"
     ]
    }
   ],
   "source": [
    "model = Linear('fc1', 3, 1)\n",
    "init_weight = torch.zeros([1, 3])\n",
    "init_bias = torch.zeros([])\n",
    "for p in model.param():\n",
    "    print(p)\n",
    "    for key, param in p.items():\n",
    "        print(\"Parameter containing:\")\n",
    "        if key == 'weight': \n",
    "            print(param.data.t())\n",
    "            init_weight = param.data.t().clone()\n",
    "        if key == 'bias': \n",
    "            print(param.data)\n",
    "            init_bias = param.data.clone()\n",
    "\n",
    "input = torch.Tensor([[2., 4., 6.], [1., 4., 6.]])\n",
    "output = model(input)\n",
    "target = torch.Tensor([[2], [1]])\n",
    "\n",
    "criterion = MSELoss()\n",
    "optimizer = SGD(model)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "nb_epochs = 5\n",
    "for e in range(nb_epochs):\n",
    "    print()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "    print(\"e = {}, loss = {}\".format(e, loss))\n",
    "    grad_loss_wrt_output = criterion.backward()\n",
    "    print(\"grad_loss_wrt_output = \", grad_loss_wrt_output.t())\n",
    "    model.backward(grad_loss_wrt_output)\n",
    "    optimizer.step(criterion(output, target))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "torch.set_grad_enabled(True)\n",
    "input = torch.Tensor([[2., 4., 6.], [1., 4., 6.]])\n",
    "target = torch.Tensor([[2], [1]])\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(3, 1)\n",
    "        self.fc.weight.data = init_weight\n",
    "        self.fc.bias.data = init_bias\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# I want to print the gradient of the loss wrt to the model output. For this I register a backward hook.\n",
    "def hook(module, gradInput, gradOutput):\n",
    "    for grad in gradOutput:\n",
    "        print(\"grad_loss_wrt_output = \", grad.t())\n",
    "\n",
    "model_torch = Net()\n",
    "for p in model_torch.parameters():\n",
    "    print(p)\n",
    "model_torch.register_backward_hook(hook)     \n",
    "criterion_torch = nn.MSELoss()\n",
    "optimizer_torch = torch.optim.SGD(model_torch.parameters(), lr=0.01)\n",
    "for e in range(nb_epochs):\n",
    "    print()\n",
    "    optimizer_torch.zero_grad()\n",
    "    output = model_torch(input).requires_grad_()\n",
    "    loss_torch = criterion_torch(output, target)\n",
    "    print(\"e = {}, loss = {}\".format(e, loss_torch))\n",
    "    loss_torch.backward()\n",
    "    for p in model_torch.parameters():\n",
    "        print(\"weight.grad = \", p.grad)\n",
    "        break\n",
    "    optimizer_torch.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding module = fc1 to children\n",
      "adding child =  <__main__.Linear object at 0x7f86b07d8550>\n",
      "Adding module = fc2 to children\n",
      "adding child =  <__main__.Linear object at 0x7f86b07d8630>\n",
      "Adding module = relu1 to children\n",
      "adding child =  <__main__.ReLU object at 0x7f86b07d86d8>\n",
      "Parameters of module  fc1\n",
      "Parameters of module  fc2\n",
      "Parameters of module  relu1\n",
      "tensor([[ 0.3424,  0.5196, -0.5871, -0.3771]])\n",
      "\n",
      "IN ZERO_GRAD, param_dict =  <generator object Module.param at 0x7f86b0810ed0>\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7f86b07d85c0>), ('bias', <__main__.Parameter object at 0x7f86b07d85f8>)])\n",
      "IN ZERO_GRAD, param_dict =  <generator object Module.param at 0x7f86b0810ed0>\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7f86b07d8668>), ('bias', <__main__.Parameter object at 0x7f86b07d86a0>)])\n",
      "IN ZERO_GRAD, param_dict =  <generator object Module.param at 0x7f86b0810ed0>\n",
      "OrderedDict()\n",
      "e = 0, output = tensor([[2.9778],\n",
      "        [1.9408]])\n",
      "e = 0, loss = 0.920551061630249\n",
      "grad_loss_wrt_output =  tensor([[0.9778, 0.9408]])\n",
      "In Seq backward, grad_output =  (tensor([[0.9778],\n",
      "        [0.9408]]),)\n",
      "In Seq backward, grad_output =  tensor([[0.9778],\n",
      "        [0.9408]])\n",
      "In Seq backward, grad_output =  tensor([[0.9778],\n",
      "        [0.9408]])\n",
      "In Linear backward, grad_ouput =  tensor([[0.9778],\n",
      "        [0.9408]])\n",
      "self.weight.data.t() =  tensor([[ 0.3424,  0.5196, -0.5871, -0.3771]])\n",
      "torch.Size([2, 1])\n",
      "In Seq backward, grad_output =  tensor([[ 0.3348,  0.5081, -0.5741, -0.3687],\n",
      "        [ 0.3221,  0.4889, -0.5524, -0.3548]])\n",
      "In Seq backward, grad_output =  tensor([[ 0.3348,  0.5081, -0.5741, -0.3687],\n",
      "        [ 0.3221,  0.4889, -0.5524, -0.3548]])\n",
      "In Linear backward, grad_ouput =  tensor([[ 0.3348,  0.5081, -0.5741, -0.3687],\n",
      "        [ 0.3221,  0.4889, -0.5524, -0.3548]])\n",
      "self.weight.data.t() =  tensor([[-0.0075, -0.3852, -0.0887],\n",
      "        [ 0.5364,  0.2682,  0.2646],\n",
      "        [-0.8230, -0.0198, -0.3022],\n",
      "        [-0.7359,  0.7929, -0.1966]])\n",
      "torch.Size([2, 4])\n",
      "In Seq backward, grad_output =  tensor([[ 1.0139, -0.2737,  0.3507],\n",
      "        [ 0.9756, -0.2633,  0.3374]])\n",
      "IN STEP, param_dict =  <generator object Module.param at 0x7f86b0810e58>\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7f86b07d85c0>), ('bias', <__main__.Parameter object at 0x7f86b07d85f8>)])\n",
      "IN STEP, param_dict =  <generator object Module.param at 0x7f86b0810e58>\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7f86b07d8668>), ('bias', <__main__.Parameter object at 0x7f86b07d86a0>)])\n",
      "IN STEP, param_dict =  <generator object Module.param at 0x7f86b0810e58>\n",
      "OrderedDict()\n",
      "\n",
      "IN ZERO_GRAD, param_dict =  <generator object Module.param at 0x7f86b0810e58>\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7f86b07d85c0>), ('bias', <__main__.Parameter object at 0x7f86b07d85f8>)])\n",
      "IN ZERO_GRAD, param_dict =  <generator object Module.param at 0x7f86b0810e58>\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7f86b07d8668>), ('bias', <__main__.Parameter object at 0x7f86b07d86a0>)])\n",
      "IN ZERO_GRAD, param_dict =  <generator object Module.param at 0x7f86b0810e58>\n",
      "OrderedDict()\n",
      "e = 1, output = tensor([[1.4419],\n",
      "        [0.5014]])\n",
      "e = 1, loss = 0.2800452709197998\n",
      "grad_loss_wrt_output =  tensor([[-0.5581, -0.4986]])\n",
      "In Seq backward, grad_output =  (tensor([[-0.5581],\n",
      "        [-0.4986]]),)\n",
      "In Seq backward, grad_output =  tensor([[-0.5581],\n",
      "        [-0.4986]])\n",
      "In Seq backward, grad_output =  tensor([[-0.5581],\n",
      "        [-0.4986]])\n",
      "In Linear backward, grad_ouput =  tensor([[-0.5581],\n",
      "        [-0.4986]])\n",
      "self.weight.data.t() =  tensor([[ 0.4007,  0.4658, -0.5191, -0.3947]])\n",
      "torch.Size([2, 1])\n",
      "In Seq backward, grad_output =  tensor([[-0.2236, -0.2600,  0.2897,  0.2203],\n",
      "        [-0.1998, -0.2322,  0.2588,  0.1968]])\n",
      "In Seq backward, grad_output =  tensor([[-0.2236, -0.2600,  0.2897,  0.2203],\n",
      "        [-0.1998, -0.2322,  0.2588,  0.1968]])\n",
      "In Linear backward, grad_ouput =  tensor([[-0.2236, -0.2600,  0.2897,  0.2203],\n",
      "        [-0.1998, -0.2322,  0.2588,  0.1968]])\n",
      "self.weight.data.t() =  tensor([[-0.0174, -0.4114, -0.1282],\n",
      "        [ 0.5214,  0.2283,  0.2048],\n",
      "        [-0.8060,  0.0252, -0.2346],\n",
      "        [-0.7250,  0.8218, -0.1532]])\n",
      "torch.Size([2, 4])\n",
      "In Seq backward, grad_output =  tensor([[-0.5249,  0.2210, -0.1263],\n",
      "        [-0.4689,  0.1975, -0.1128]])\n",
      "IN STEP, param_dict =  <generator object Module.param at 0x7f86b0810ed0>\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7f86b07d85c0>), ('bias', <__main__.Parameter object at 0x7f86b07d85f8>)])\n",
      "IN STEP, param_dict =  <generator object Module.param at 0x7f86b0810ed0>\n",
      "OrderedDict([('weight', <__main__.Parameter object at 0x7f86b07d8668>), ('bias', <__main__.Parameter object at 0x7f86b07d86a0>)])\n",
      "IN STEP, param_dict =  <generator object Module.param at 0x7f86b0810ed0>\n",
      "OrderedDict()\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([4, 3])\n",
      "Parameter containing:\n",
      "tensor([[-0.0075, -0.3852, -0.0887],\n",
      "        [ 0.5364,  0.2682,  0.2646],\n",
      "        [-0.8230, -0.0198, -0.3022],\n",
      "        [-0.7359,  0.7929, -0.1966]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.9553, -0.6623, -0.4122,  0.0370], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3424,  0.5196, -0.5871, -0.3771]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3146], requires_grad=True)\n",
      "\n",
      "e = 0, output = tensor([[2.9778],\n",
      "        [1.9408]], grad_fn=<ThresholdBackward0>)\n",
      "e = 0, loss = 0.920551061630249\n",
      "grad_loss_wrt_output =  tensor([[0.9778, 0.9408]])\n",
      "\n",
      "e = 1, output = tensor([[1.4419],\n",
      "        [0.5014]], grad_fn=<ThresholdBackward0>)\n",
      "e = 1, loss = 0.2800452709197998\n",
      "grad_loss_wrt_output =  tensor([[-0.5581, -0.4986]])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "torch.manual_seed(0)\n",
    "torch.set_grad_enabled(False)\n",
    "del nn\n",
    "del optim\n",
    "model_seq = Sequential(\n",
    "    Linear('fc1', 3, 4),\n",
    "    Linear('fc2', 4, 1),\n",
    "    ReLU('relu1')\n",
    "    )\n",
    "\n",
    "input = torch.Tensor([[2., 4., 6.], [1., 4., 6.]])\n",
    "target = torch.Tensor([[2], [1]])\n",
    "model_seq(input)\n",
    "\n",
    "def get_initial_parameters(model):\n",
    "    i = 0\n",
    "    for p in model_seq.param(verbose=True): # loop over generator object of each module\n",
    "        for param_dict in p: # get parameters OrderedDict()\n",
    "            if param_dict is not None:\n",
    "                for key, param in param_dict.items(): # loop over the parameters OrderedDict()\n",
    "                    if key == 'weight':\n",
    "                        if i == 0: init_weight1 = param.data.t().clone()\n",
    "                        if i == 1: init_weight2 = param.data.t().clone()    \n",
    "                    if key == 'bias':\n",
    "                        if i == 0: \n",
    "                            init_bias1 = param.data.clone()\n",
    "                            i += 1\n",
    "                        if i == 1: init_bias2 = param.data.clone()\n",
    "    return init_weight1, init_bias1, init_weight2, init_bias2\n",
    "init_weight1, init_bias1, init_weight2, init_bias2 = get_initial_parameters(model_seq)\n",
    "print(init_weight2)                       \n",
    "\n",
    "                    \n",
    "criterion = MSELoss()\n",
    "optimizer = SGD(model_seq)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "nb_epochs = 2\n",
    "for e in range(nb_epochs):\n",
    "    print()\n",
    "    optimizer.zero_grad()\n",
    "    output = model_seq(input)\n",
    "    print(\"e = {}, output = {}\".format(e, output))\n",
    "    loss = criterion(output, target)\n",
    "    print(\"e = {}, loss = {}\".format(e, loss))\n",
    "    \n",
    "    grad_loss_wrt_output = criterion.backward()\n",
    "    print(\"grad_loss_wrt_output = \", grad_loss_wrt_output.t())\n",
    "    \n",
    "    model_seq.backward(grad_loss_wrt_output)\n",
    "    optimizer.step(criterion(output, target))\n",
    "    \n",
    "print()\n",
    "print()\n",
    "print()\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "torch.set_grad_enabled(True)\n",
    "input = torch.Tensor([[2., 4., 6.], [1., 4., 6.]])\n",
    "target = torch.Tensor([[2], [1]])\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 4)\n",
    "        \n",
    "        print(self.fc1.weight.data.shape)\n",
    "        self.fc1.weight.data = init_weight1\n",
    "        \n",
    "        self.fc2 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc1.bias.data = init_bias1\n",
    "        self.fc2.weight.data = init_weight2\n",
    "        self.fc2.bias.data = init_bias2\n",
    "    def forward(self, x):\n",
    "        #print(\"x0 = \", x)\n",
    "        x = self.fc1(x)\n",
    "        #print(\"x1 = \", x)\n",
    "        x = self.fc2(x)\n",
    "        #print(\"x2 = \", x)\n",
    "        x = self.relu(x)\n",
    "        #print(\"x3 = \", x)\n",
    "        return x\n",
    "    \n",
    "# I want to print the gradient of the loss wrt to the model output. For this I register a backward hook.\n",
    "def hook(module, gradInput, gradOutput):\n",
    "    for grad in gradOutput:\n",
    "        print(\"grad_loss_wrt_output = \", grad.t())\n",
    "        \n",
    "model_torch = Net()\n",
    "for p in model_torch.parameters():\n",
    "    print(p)\n",
    "model_torch.register_backward_hook(hook)     \n",
    "criterion_torch = nn.MSELoss()\n",
    "optimizer_torch = torch.optim.SGD(model_torch.parameters(), lr=0.01)\n",
    "for e in range(nb_epochs):\n",
    "    print()\n",
    "    optimizer_torch.zero_grad()\n",
    "    output = model_torch(input).requires_grad_()\n",
    "    print(\"e = {}, output = {}\".format(e, output))\n",
    "    loss_torch = criterion_torch(output, target)\n",
    "    print(\"e = {}, loss = {}\".format(e, loss_torch))\n",
    "    loss_torch.backward()\n",
    "    #for p in model_torch.parameters():\n",
    "    #    print(p)\n",
    "    #    print(\"weight.grad = \", p.grad)\n",
    "    #    break\n",
    "    optimizer_torch.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0977, -0.5391, -0.4172],\n",
      "        [-0.2976,  0.3643,  0.3385],\n",
      "        [-0.2561, -0.0208,  0.3693],\n",
      "        [ 0.5740,  0.2291,  0.0780]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3871, -0.3399,  0.1076, -0.4476], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3465, -0.2583,  0.2262,  0.2011]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2962], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "torch.set_grad_enabled(True)\n",
    "model_seq_torch = nn.Sequential(nn.Linear(3, 4), \n",
    "                            nn.Linear(4, 1), \n",
    "                            nn.ReLU())\n",
    "model_seq_torch(input)\n",
    "for p in model_seq_torch.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODULE.PARAM\n",
      "module.param() =  <generator object Module.param at 0x7f86b07b81b0>\n",
      "module.param() =  <generator object Module.param at 0x7f86b07b81b0>\n",
      "module.param() =  <generator object Module.param at 0x7f86b07b81b0>\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"MODULE.PARAM\")\n",
    "for key, module in model_seq._children.items():\n",
    "    print(\"module.param() = \", module.param())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second project, do we first accumulate the gradient then afterwards calculate the derivate of the loss wrt \n",
    "to the input.  Or do it the other way around.\n",
    "They are usually unrelated computations. Think about the following scenario. You have a batch of inputs x_0 to x_9. \n",
    "And a single parameter a. Thus the forward pass for this module is s_i = a*x_i. For the backward pass we get as \n",
    "input dl/ds_i for all i and we need to compute dl/da and dl/dx_i . It is quite obvious that \n",
    "dl/da = sum x_i * dl/ds_i for all i. And dl/dx_i = dl/ds_i * a. The order in which one computes the two is irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINTING PARAMETERS\n",
      "p =  Parameter containing:\n",
      "tensor([[ 0.2136,  0.3882],\n",
      "        [-0.0892,  0.0270],\n",
      "        [ 0.1638,  0.4387],\n",
      "        [ 0.6790, -0.5449],\n",
      "        [-0.2591,  0.2779],\n",
      "        [ 0.5859,  0.6153],\n",
      "        [ 0.6239,  0.1407],\n",
      "        [-0.6149,  0.0650],\n",
      "        [-0.4424, -0.6590],\n",
      "        [ 0.6283,  0.5377]], requires_grad=True)\n",
      "p =  Parameter containing:\n",
      "tensor([-0.7054,  0.1324, -0.1191, -0.1164, -0.3237,  0.2719, -0.4188,  0.2592,\n",
      "         0.3576,  0.5062], requires_grad=True)\n",
      "PRINTING PREDICTION\n",
      "y_pred =  tensor([[0.2846, 0.0971, 0.9221, 0.0000, 0.0000, 2.0885, 0.4865, 0.0000, 0.0000,\n",
      "         2.2098],\n",
      "        [0.1101, 0.0000, 0.6472, 0.6966, 0.0000, 2.0590, 0.9697, 0.0000, 0.0000,\n",
      "         2.3004],\n",
      "        [1.4882, 0.0000, 2.1271, 0.0000, 0.0105, 4.4909, 2.0158, 0.0000, 0.0000,\n",
      "         4.5416]], grad_fn=<ThresholdBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a2c02554393a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2153\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "x = torch.tensor([[1, 2], [2, 1], [3, 4]]).type(torch.FloatTensor).requires_grad_()\n",
    "y = torch.tensor([1, 0.4, 3])\n",
    "#x = torch.tensor([[1., 2.]]).requires_grad_()\n",
    "#y = torch.tensor([1.])\n",
    "\n",
    "model = nn.Sequential(nn.Linear(2, 10), nn.ReLU())\n",
    "\n",
    "print(\"PRINTING PARAMETERS\")\n",
    "for p in model.parameters():\n",
    "    print(\"p = \", p)\n",
    "y_pred = model(x)\n",
    "\n",
    "print(\"PRINTING PREDICTION\")\n",
    "print(\"y_pred = \", y_pred)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "loss = criterion(y_pred, y)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"PRINTING GRADIENT\")\n",
    "#print(\"loss.grad = \", autograd.grad(loss, x))\n",
    "loss.backward()\n",
    "for p in model.parameters():\n",
    "    print(\"p.grad = \", p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
